{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup config\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING,  # default to WARNING\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"SlopRankLogger\")\n",
    "logger.setLevel(logging.INFO)  # Our SlopRank logs at INFO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Configuration (EvalConfig)\n",
    "\n",
    "@dataclass\n",
    "class EvalConfig:\n",
    "    \"\"\"Configuration for the evaluation system.\"\"\"\n",
    "    model_names: List[str]\n",
    "    evaluation_method: int  # e.g., 1 => numeric rating\n",
    "    use_subset_evaluation: bool\n",
    "    evaluators_subset_size: int\n",
    "    output_dir: Path\n",
    "    request_delay: float = 0.0  # adjustable delay between requests if needed\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        if self.evaluation_method not in {1, 2}:\n",
    "            raise ValueError(\"evaluation_method must be 1 or 2\")\n",
    "        if self.evaluators_subset_size >= len(self.model_names):\n",
    "            raise ValueError(\"evaluators_subset_size must be < number of models\")\n",
    "\n",
    "DEFAULT_CONFIG = EvalConfig(\n",
    "    model_names=[\n",
    "        \"gemini-2.0-flash-thinking-exp-1219\",\n",
    "        \"gemini-exp-1206\",\n",
    "        \"claude-3-5-sonnet-latest\",\n",
    "        \"o1-preview\",\n",
    "        \"gpt-4o\",\n",
    "        \"deepseek-chat\"\n",
    "    ],\n",
    "    evaluation_method=1,  # numeric\n",
    "    use_subset_evaluation=False,\n",
    "    evaluators_subset_size=3,\n",
    "    output_dir=Path(\"results\"),  # folder for CSV outputs\n",
    "    request_delay=0.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 14:22:29,563 - INFO - Reading prompts from prompts.xlsx ...\n",
      "2025-01-09 14:22:29,612 - INFO - Loaded 5 prompts from Excel.\n"
     ]
    }
   ],
   "source": [
    "# 3. Read prompts\n",
    "# We assume you have a local \"prompts.xlsx\" file with columns [\"Questions\", \"Answer_key\"].\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # if you have .env credentials\n",
    "\n",
    "logger.info(\"Reading prompts from prompts.xlsx ...\")\n",
    "prompts_df = pd.read_excel(\"prompts.xlsx\", sheet_name=0)\n",
    "prompts = prompts_df[\"Questions\"].tolist()\n",
    "\n",
    "# If \"Answer_key\" column exists, read it; otherwise fallback to None\n",
    "if \"Answer_key\" in prompts_df.columns:\n",
    "    answer_keys = prompts_df[\"Answer_key\"].tolist()\n",
    "else:\n",
    "    logger.warning(\"No Answer_key column found; using None.\")\n",
    "    answer_keys = [None]*len(prompts_df)\n",
    "\n",
    "prompt_pairs = list(zip(prompts, answer_keys))\n",
    "logger.info(f\"Loaded {len(prompt_pairs)} prompts from Excel.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Collecting the responses\n",
    "\n",
    "def collect_responses(prompt_pairs: List[Tuple[str, str]], config: EvalConfig, llm_module) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Query each model with each prompt, saving the raw answers.\n",
    "    Return a DataFrame: (prompt, model, response, is_valid, response_time, Answer_key).\n",
    "    \"\"\"\n",
    "    logger.info(\"Collecting responses...\")\n",
    "    responses = []\n",
    "    total_start = time.time()\n",
    "\n",
    "    # For each (prompt, answer_key) pair\n",
    "    for i, (prompt, answer_key) in enumerate(prompt_pairs, 1):\n",
    "        logger.info(f\"Processing prompt {i}/{len(prompt_pairs)}\")\n",
    "        for model_name in config.model_names:\n",
    "            start_time = time.time()\n",
    "            logger.info(f\"Querying {model_name}...\")\n",
    "            try:\n",
    "                model = llm_module.get_model(model_name)\n",
    "                raw_response = model.prompt(prompt).text()\n",
    "                # Simple validation: ensure at least 10 chars\n",
    "                valid = isinstance(raw_response, str) and len(raw_response.strip()) >= 10\n",
    "                elapsed = time.time() - start_time\n",
    "                tokens_used = len(raw_response.split())\n",
    "\n",
    "                responses.append({\n",
    "                    'prompt': prompt,\n",
    "                    'model': model_name,\n",
    "                    'response': raw_response if valid else None,\n",
    "                    'is_valid': valid,\n",
    "                    'response_time': elapsed,\n",
    "                    'Answer_key': answer_key,\n",
    "                    'token_count': tokens_used \n",
    "                })\n",
    "                logger.info(\n",
    "                    f\"{model_name} responded in {elapsed:.2f}s - {'Valid' if valid else 'Invalid'}\"\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                elapsed = time.time() - start_time\n",
    "                logger.error(f\"Error from {model_name} after {elapsed:.2f}s: {str(e)}\")\n",
    "                responses.append({\n",
    "                    'prompt': prompt,\n",
    "                    'model': model_name,\n",
    "                    'response': None,\n",
    "                    'is_valid': False,\n",
    "                    'response_time': elapsed,\n",
    "                    'Answer_key': answer_key,\n",
    "                    'token_count': 0\n",
    "                })\n",
    "\n",
    "            if config.request_delay > 0.0:\n",
    "                time.sleep(config.request_delay)\n",
    "\n",
    "    total_time = time.time() - total_start\n",
    "    logger.info(f\"All responses collected in {total_time:.2f}s\")\n",
    "\n",
    "    return pd.DataFrame(responses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Collecting Raw Evaluations (Unparsed)\n",
    "\n",
    "def collect_raw_evaluations(responses_df: pd.DataFrame, config: EvalConfig, llm_module) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Each model in config.model_names evaluates (rates) the others' responses.\n",
    "    We do NOT parse the JSON here.\n",
    "    Instead, we store the raw text in a DataFrame so we can debug if parsing fails.\n",
    "    \"\"\"\n",
    "    logger.info(\"Collecting raw evaluations (unparsed)...\")\n",
    "    raw_judgment_log = []\n",
    "\n",
    "    unique_prompts = responses_df['prompt'].unique()\n",
    "    for prompt in unique_prompts:\n",
    "        # Subset for that prompt\n",
    "        prompt_subset = responses_df[responses_df['prompt'] == prompt]\n",
    "        answer_key = prompt_subset['Answer_key'].iloc[0] if 'Answer_key' in prompt_subset.columns else None\n",
    "\n",
    "        # model_name -> text response\n",
    "        prompt_responses = prompt_subset.set_index('model')['response'].to_dict()\n",
    "\n",
    "        # Each model is the judge\n",
    "        for judge_model in config.model_names:\n",
    "            # Exclude judge's own or missing responses\n",
    "            other_models = [\n",
    "                m for m in config.model_names\n",
    "                if m != judge_model and prompt_responses.get(m) is not None\n",
    "            ]\n",
    "\n",
    "            if config.use_subset_evaluation and other_models:\n",
    "                other_models = random.sample(\n",
    "                    other_models,\n",
    "                    min(config.evaluators_subset_size, len(other_models))\n",
    "                )\n",
    "\n",
    "            if not other_models:\n",
    "                continue\n",
    "\n",
    "            # Create a prompt that each judge will see\n",
    "            model_to_anon = {m: f\"Model_{i+1}\" for i, m in enumerate(other_models)}\n",
    "            answers_section = \"\\n\".join([\n",
    "                f\"{model_to_anon[m]}:\\n{prompt_responses[m]}\\n---\"\n",
    "                for m in other_models\n",
    "            ])\n",
    "            if answer_key:\n",
    "                answer_section = f\"The Answer Key here is:\\n{answer_key}\\n---\\n\"\n",
    "            else:\n",
    "                answer_section = \"\"\n",
    "\n",
    "            instructions = f\"\"\"\n",
    "IMPORTANT: Your job is to evaluate the given problem and return a complete and syntactically perfect JSON object with ratings. Format should be: {{\"Model_1\": 8, \"Model_2\": 7}}\n",
    "\n",
    "Rate these responses to: \"{prompt}\"\n",
    "{answer_section}\n",
    "\n",
    "The answers to rank from other LLMs:\n",
    "{answers_section}\n",
    "\n",
    "Rate each 1-10 based on: accuracy, completeness, clarity, relevance.\n",
    "10: Only for truly exceptional, world leading class\n",
    "8-9: Excellent, like a top professional in the field,\n",
    "6-7: Good enough, like a mediocre undergraduate student, \n",
    "4-5: Fair, like an okay enough high schooler,\n",
    "1-3: Poor\n",
    "\n",
    "Again, the format to follow diligently: {{\"Model_1\": 8, \"Model_2\": 7}}\n",
    "\"\"\".strip()\n",
    "\n",
    "            try:\n",
    "                judge_llm = llm_module.get_model(judge_model)\n",
    "                judge_result_obj = judge_llm.prompt(instructions)\n",
    "                raw_judgment = judge_result_obj.text()\n",
    "\n",
    "                raw_judgment_tokens = len(raw_judgment.split())\n",
    "                raw_judgment_log.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"judge_model\": judge_model,\n",
    "                    \"raw_judgment\": raw_judgment,\n",
    "                    \"model_mapping\": json.dumps(model_to_anon),\n",
    "                    \"raw_judgment_token_count\": raw_judgment_tokens   # store as JSON\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error collecting raw eval from judge={judge_model}: {str(e)}\")\n",
    "                # Save partial record so we know which judge/prompt failed\n",
    "                raw_judgment_log.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"judge_model\": judge_model,\n",
    "                    \"raw_judgment\": None,\n",
    "                    \"model_mapping\": json.dumps(model_to_anon),\n",
    "                    \"raw_judgment_token_count\": 0,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "\n",
    "    raw_eval_df = pd.DataFrame(raw_judgment_log)\n",
    "    logger.info(\"Finished collecting raw evaluation outputs.\")\n",
    "    return raw_eval_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Parsing the raw evaluations\n",
    "\n",
    "def parse_evaluation_rows(raw_eval_df: pd.DataFrame, config: EvalConfig) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse each row of the raw_eval_df, which contains judge's raw JSON-like output.\n",
    "    If parsing fails, fallback to a default rating (4.1) for each rated model.\n",
    "    \n",
    "    Returns a DataFrame: (prompt, judge_model, rated_model, score).\n",
    "    \"\"\"\n",
    "    evaluations = []\n",
    "\n",
    "    for _, row in raw_eval_df.iterrows():\n",
    "        prompt = row[\"prompt\"]\n",
    "        judge_model = row[\"judge_model\"]\n",
    "        raw_judgment = row[\"raw_judgment\"]\n",
    "        raw_judgment_tokens = row.get(\"raw_judgment_token_count\", 0)\n",
    "\n",
    "        # Convert model_mapping from JSON string back to dict\n",
    "        try:\n",
    "            model_mapping = json.loads(row[\"model_mapping\"])  # e.g. {\"gemini-exp-1206\":\"Model_1\"}\n",
    "        except:\n",
    "            model_mapping = {}\n",
    "\n",
    "        if not raw_judgment:\n",
    "            # If there's no raw judgment at all, we might skip or fallback\n",
    "            for real_model in model_mapping.keys():\n",
    "                evaluations.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"judge_model\": judge_model,\n",
    "                    \"rated_model\": real_model,\n",
    "                    \"score\": 4.1,           # << changed fallback\n",
    "                    \"parse_failed\": True\n",
    "                })\n",
    "            logger.warning(f\"No raw_judgment for prompt={prompt}, judge={judge_model}; skipping parse.\")\n",
    "            continue\n",
    "\n",
    "        # Try to parse a JSON object from the raw_judgment\n",
    "        try:\n",
    "            start = raw_judgment.find(\"{\")\n",
    "            end = raw_judgment.rfind(\"}\") + 1\n",
    "\n",
    "            if start == -1 or end == 0:\n",
    "                raise ValueError(\"No JSON object found in raw_judgment\")\n",
    "\n",
    "            data = json.loads(raw_judgment[start:end])\n",
    "            # Reverse mapping: \"Model_1\" => \"gemini-exp-1206\"\n",
    "            anon_to_real = {v: k for k, v in model_mapping.items()}\n",
    "\n",
    "            for anon_id, score in data.items():\n",
    "                real_model = anon_to_real.get(anon_id)\n",
    "                if not real_model:\n",
    "                    # If we can't find the real model name, skip\n",
    "                    continue\n",
    "                numeric_score = float(score)\n",
    "                numeric_score = max(1.0, min(10.0, numeric_score))  # clamp 1..10\n",
    "\n",
    "                evaluations.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"judge_model\": judge_model,\n",
    "                    \"rated_model\": real_model,\n",
    "                    \"score\": numeric_score,\n",
    "                    \"parse_failed\": False,\n",
    "                    \"raw_judgment_token_count\": raw_judgment_tokens\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Parsing error for judge={judge_model}, prompt={prompt}: {str(e)}\")\n",
    "            # If parse fails, assign a default rating\n",
    "            for real_model in model_mapping.keys():\n",
    "                evaluations.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"judge_model\": judge_model,\n",
    "                    \"rated_model\": real_model,\n",
    "                    \"score\": 4.1,\n",
    "                    \"parse_failed\": True,\n",
    "                    \"raw_judgment_token_count\": raw_judgment_tokens\n",
    "                })\n",
    "\n",
    "    evals_df = pd.DataFrame(evaluations)\n",
    "    return evals_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 14:22:57,267 - INFO - Using config: EvalConfig(model_names=['gemini-2.0-flash-thinking-exp-1219', 'gemini-exp-1206', 'claude-3-5-sonnet-latest', 'o1-preview', 'gpt-4o', 'deepseek-chat'], evaluation_method=1, use_subset_evaluation=False, evaluators_subset_size=3, output_dir=PosixPath('results'), request_delay=0.0)\n",
      "2025-01-09 14:22:57,269 - INFO - Loading existing responses from results/responses.csv\n",
      "2025-01-09 14:22:57,294 - INFO - No raw_evaluations.csv found; collecting now (unparsed).\n",
      "2025-01-09 14:22:57,294 - INFO - Collecting raw evaluations (unparsed)...\n",
      "2025-01-09 14:49:01,107 - INFO - Finished collecting raw evaluation outputs.\n",
      "2025-01-09 14:49:01,115 - INFO - Saved raw evaluations to results/raw_evaluations.csv\n",
      "2025-01-09 14:49:01,116 - INFO - Loading parsed evaluations from results/evaluations.csv\n",
      "2025-01-09 14:49:01,122 - INFO - Here are the first few rows of the parsed evaluations:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>judge_model</th>\n",
       "      <th>rated_model</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Provide a summary of the gene TM2D2, including...</td>\n",
       "      <td>gemini-2.0-flash-thinking-exp-1219</td>\n",
       "      <td>deepseek-chat</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Provide a summary of the gene TM2D2, including...</td>\n",
       "      <td>gemini-2.0-flash-thinking-exp-1219</td>\n",
       "      <td>o1-preview</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Provide a summary of the gene TM2D2, including...</td>\n",
       "      <td>gemini-2.0-flash-thinking-exp-1219</td>\n",
       "      <td>gemini-exp-1206</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Provide a summary of the gene TM2D2, including...</td>\n",
       "      <td>gemini-exp-1206</td>\n",
       "      <td>o1-preview</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Provide a summary of the gene TM2D2, including...</td>\n",
       "      <td>gemini-exp-1206</td>\n",
       "      <td>gemini-2.0-flash-thinking-exp-1219</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Provide a summary of the gene TM2D2, including...   \n",
       "1  Provide a summary of the gene TM2D2, including...   \n",
       "2  Provide a summary of the gene TM2D2, including...   \n",
       "3  Provide a summary of the gene TM2D2, including...   \n",
       "4  Provide a summary of the gene TM2D2, including...   \n",
       "\n",
       "                          judge_model                         rated_model  \\\n",
       "0  gemini-2.0-flash-thinking-exp-1219                       deepseek-chat   \n",
       "1  gemini-2.0-flash-thinking-exp-1219                          o1-preview   \n",
       "2  gemini-2.0-flash-thinking-exp-1219                     gemini-exp-1206   \n",
       "3                     gemini-exp-1206                          o1-preview   \n",
       "4                     gemini-exp-1206  gemini-2.0-flash-thinking-exp-1219   \n",
       "\n",
       "   score  \n",
       "0    5.0  \n",
       "1    5.0  \n",
       "2    5.0  \n",
       "3    8.0  \n",
       "4    7.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 7. Full workflow\n",
    "import llm  # custom LLM module\n",
    "\n",
    "# 1) Create a config\n",
    "config = DEFAULT_CONFIG\n",
    "logger.info(f\"Using config: {config}\")\n",
    "\n",
    "# 2) Collect or load responses\n",
    "resp_path = config.output_dir / \"responses.csv\"\n",
    "\n",
    "if resp_path.exists():\n",
    "    logger.info(f\"Loading existing responses from {resp_path}\")\n",
    "    responses_df = pd.read_csv(resp_path)\n",
    "else:\n",
    "    logger.info(\"No responses.csv found; collecting now from each model.\")\n",
    "    responses_df = collect_responses(prompt_pairs, config, llm)\n",
    "    responses_df.to_csv(resp_path, index=False)\n",
    "    logger.info(f\"Saved new responses to {resp_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Collect or load raw evaluations\n",
    "raw_eval_path = config.output_dir / \"raw_evaluations.csv\"\n",
    "\n",
    "if raw_eval_path.exists():\n",
    "    logger.info(f\"Loading existing raw evaluations from {raw_eval_path}\")\n",
    "    raw_eval_df = pd.read_csv(raw_eval_path)\n",
    "else:\n",
    "    logger.info(\"No raw_evaluations.csv found; collecting now (unparsed).\")\n",
    "    raw_eval_df = collect_raw_evaluations(responses_df, config, llm)\n",
    "    raw_eval_df.to_csv(raw_eval_path, index=False)\n",
    "    logger.info(f\"Saved raw evaluations to {raw_eval_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Parse or load final evaluations\n",
    "eval_path = config.output_dir / \"evaluations.csv\"\n",
    "\n",
    "if eval_path.exists():\n",
    "    logger.info(f\"Loading parsed evaluations from {eval_path}\")\n",
    "    evaluations_df = pd.read_csv(eval_path)\n",
    "else:\n",
    "    logger.info(\"No evaluations.csv found; parsing raw evaluations now.\")\n",
    "    evaluations_df = parse_evaluation_rows(raw_eval_df, config)\n",
    "    evaluations_df.to_csv(eval_path, index=False)\n",
    "    logger.info(f\"Saved parsed evaluations to {eval_path}\")\n",
    "\n",
    "\n",
    "# 10. Inspect or analyze the final numeric scores\n",
    "logger.info(\"Here are the first few rows of the parsed evaluations:\")\n",
    "display(evaluations_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
