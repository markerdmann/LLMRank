{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Introducing SlopRank\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import llm\n",
    "import networkx as nx\n",
    "import json\n",
    "import random\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "##############################################################################\n",
    "# 1. Configuration\n",
    "##############################################################################\n",
    "\n",
    "MODEL_NAMES = [\n",
    "    \"gemini-2.0-flash-thinking-exp-1219\",\n",
    "    \"gemini-exp-1206\",\n",
    "    \"claude-3-5-sonnet-latest\",\n",
    "    \"claude-3-opus-latest\",\n",
    "    \"o1-preview\",\n",
    "    \"gpt-4o\",\n",
    "    \"deepseek-chat\"\n",
    "]\n",
    "\n",
    "model_objects = {m: llm.get_model(m) for m in MODEL_NAMES}\n",
    "\n",
    "EVALUATION_METHOD = 1  # 1 (numeric 1â€“10) or 2 (Upvote/Downvote)\n",
    "USE_SUBSET_EVALUATION = False  # Toggle to use partial evaluation\n",
    "EVALUATORS_SUBSET_SIZE = 2  # If True, limit judges to evaluate a subset of models\n",
    "\n",
    "##############################################################################\n",
    "# 2. Prompting functions\n",
    "##############################################################################\n",
    "\n",
    "def query_model(prompt, model_name):\n",
    "    \"\"\"\n",
    "    Sends a prompt to a specified model via 'llm' and returns the response text.\n",
    "    \"\"\"\n",
    "    response = model_objects[model_name].prompt(prompt)\n",
    "    return response.text()\n",
    "\n",
    "def query_model_all(df, model_name):\n",
    "    \"\"\"\n",
    "    Query the chosen model for all prompts in the DataFrame and save responses.\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    cleaned_prompts = df[\"prompt\"].str.strip().str.lower()\n",
    "    colname = f\"response_{model_name}\"\n",
    "    df[colname] = cleaned_prompts.map(lambda x: query_model(x, model_name))\n",
    "    print(f\"{model_name} processing time: {time.time() - t0:.2f}s\")\n",
    "    \n",
    "    # Save responses for this model\n",
    "    df[[colname]].to_csv(f\"responses_{model_name}.csv\", index=False)\n",
    "    print(f\"Saved responses for {model_name} to responses_{model_name}.csv\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def gather_all_model_responses(raw_prompts):\n",
    "    \"\"\"\n",
    "    Gather responses from all models and save to disk incrementally.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"prompt\": raw_prompts})\n",
    "    for m in MODEL_NAMES:\n",
    "        df = query_model_all(df, m)\n",
    "    return df\n",
    "\n",
    "##############################################################################\n",
    "# 3. Evaluate the responses\n",
    "##############################################################################\n",
    "\n",
    "def evaluate_responses(df):\n",
    "    \"\"\"\n",
    "    Each model evaluates the other models' responses, builds a graph of endorsements,\n",
    "    and computes PageRank to rank models.\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(MODEL_NAMES)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        prompt = row[\"prompt\"]\n",
    "        model_responses = {m: row.get(f\"response_{m}\", \"No response\") for m in MODEL_NAMES}\n",
    "\n",
    "        for judge_model in MODEL_NAMES:\n",
    "            other_models = [m for m in MODEL_NAMES if m != judge_model]\n",
    "\n",
    "            if USE_SUBSET_EVALUATION and len(other_models) > EVALUATORS_SUBSET_SIZE:\n",
    "                other_models = random.sample(other_models, EVALUATORS_SUBSET_SIZE)\n",
    "\n",
    "            if not other_models:\n",
    "                continue\n",
    "\n",
    "            evaluation_prompt = build_evaluation_prompt(\n",
    "                EVALUATION_METHOD, prompt, judge_model, model_responses, other_models\n",
    "            )\n",
    "            raw_judgment = query_model(evaluation_prompt, judge_model)\n",
    "            parsed_judgments = parse_evaluation_output(\n",
    "                EVALUATION_METHOD, raw_judgment, other_models\n",
    "            )\n",
    "\n",
    "            for rated_model, endorsement_val in parsed_judgments.items():\n",
    "                if G.has_edge(judge_model, rated_model):\n",
    "                    G[judge_model][rated_model][\"weight\"] += endorsement_val\n",
    "                else:\n",
    "                    G.add_edge(judge_model, rated_model, weight=endorsement_val)\n",
    "\n",
    "    # Compute PageRank\n",
    "    pagerank_scores = nx.pagerank(G, weight=\"weight\")\n",
    "    ranked_models = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Save graph and rankings\n",
    "    nx.write_gml(G, \"endorsement_graph.gml\")\n",
    "    print(\"Saved endorsement graph to endorsement_graph.gml\")\n",
    "\n",
    "    with open(\"rankings.json\", \"w\") as f:\n",
    "        json.dump(ranked_models, f, indent=4)\n",
    "    print(\"Saved rankings to rankings.json\")\n",
    "\n",
    "    return G, ranked_models\n",
    "\n",
    "def build_evaluation_prompt(method, prompt, judge_model, model_responses, other_models):\n",
    "    \"\"\"\n",
    "    Build a meta-prompt that instructs 'judge_model' to evaluate the other models' answers.\n",
    "    \"\"\"\n",
    "    answers_section = \"\\n\".join([f\"**Model {om}**: {model_responses[om]}\" for om in other_models])\n",
    "\n",
    "    if method == 1:\n",
    "        instructions = f\"\"\"\n",
    "You are {judge_model}. You see the following question:\n",
    "PROMPT: \"{prompt}\"\n",
    "\n",
    "Here are other models' answers:\n",
    "\n",
    "{answers_section}\n",
    "\n",
    "Give each model a rating from 1 to 10, strictly in JSON format with no extra text.\n",
    "Example:\n",
    "{{\n",
    "    \"modelA\": 8,\n",
    "    \"modelB\": 5\n",
    "}}\n",
    "\"\"\"\n",
    "    else:\n",
    "        instructions = f\"\"\"\n",
    "You are {judge_model}. You see the following question:\n",
    "PROMPT: \"{prompt}\"\n",
    "\n",
    "Here are other models' answers:\n",
    "\n",
    "{answers_section}\n",
    "\n",
    "Simply say \"Upvote\" or \"Downvote\" for each model, in JSON format, no extra text.\n",
    "Example:\n",
    "{{\n",
    "    \"modelA\": \"Upvote\",\n",
    "    \"modelB\": \"Downvote\"\n",
    "}}\n",
    "\"\"\"\n",
    "    return instructions.strip()\n",
    "\n",
    "def parse_evaluation_output(method, raw_judgment, other_models):\n",
    "    \"\"\"\n",
    "    Parse the raw text from the judge model into a dict of endorsements.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = json.loads(raw_judgment)\n",
    "    except:\n",
    "        if method == 1:\n",
    "            return {m: 5.0 for m in other_models}  # fallback\n",
    "        else:\n",
    "            return {m: 0 for m in other_models}    # fallback\n",
    "\n",
    "    endorsement_map = {}\n",
    "    for m in other_models:\n",
    "        val = data.get(m, None)\n",
    "        if method == 1:\n",
    "            try:\n",
    "                score = float(val)\n",
    "                endorsement_map[m] = max(1.0, min(10.0, score))\n",
    "            except:\n",
    "                endorsement_map[m] = 5.0\n",
    "        else:\n",
    "            if isinstance(val, str) and val.lower().strip() == \"upvote\":\n",
    "                endorsement_map[m] = 1\n",
    "            elif isinstance(val, str) and val.lower().strip() == \"downvote\":\n",
    "                endorsement_map[m] = 0\n",
    "            else:\n",
    "                endorsement_map[m] = 0\n",
    "\n",
    "    return endorsement_map\n",
    "\n",
    "##############################################################################\n",
    "# 4. Main\n",
    "##############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prompts_df = pd.read_csv(\"prompts.csv\")  # Assuming the CSV has a column named \"Questions\"\n",
    "    raw_prompts = prompts_df[\"Questions\"].tolist()\n",
    "    # raw_prompts = [prompts_df[\"Questions\"].iloc[0]]  # Use only the first prompt\n",
    "\n",
    "    # Gather responses for each model\n",
    "    df_responses = gather_all_model_responses(raw_prompts)\n",
    "\n",
    "    # Evaluate responses and compute rankings\n",
    "    G, ranked = evaluate_responses(df_responses)\n",
    "\n",
    "    print(\"\\n=== PageRank Scores ===\")\n",
    "    for model, score in ranked:\n",
    "        print(f\"{model}: {score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
