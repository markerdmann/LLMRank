{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inimitable SlopRank\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import llm\n",
    "import networkx as nx\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "##############################################################################\n",
    "# 1. Configuration\n",
    "##############################################################################\n",
    "\n",
    "MODEL_NAMES = [\n",
    "    \"gemini-2.0-flash-thinking-exp-1219\",\n",
    "    \"gemini-exp-1206\",\n",
    "    \"claude-3-5-sonnet-latest\",\n",
    "    \"claude-3-opus-latest\",\n",
    "    \"o1-preview\",\n",
    "    \"gpt-4o\",\n",
    "    \"deepseek-chat\"\n",
    "]\n",
    "\n",
    "model_objects = {m: llm.get_model(m) for m in MODEL_NAMES}\n",
    "\n",
    "EVALUATION_METHOD = 1  # 1 (numeric 1â€“10) or 2 (Upvote/Downvote)\n",
    "USE_SUBSET_EVALUATION = True  # Toggle to use partial evaluation\n",
    "EVALUATORS_SUBSET_SIZE = 3  # If True, limit judges to evaluate a subset of models\n",
    "\n",
    "##############################################################################\n",
    "# 2. Prompting functions\n",
    "##############################################################################\n",
    "\n",
    "def query_model(prompt, model_name):\n",
    "    \"\"\"\n",
    "    Sends a prompt to a specified model via 'llm' and returns the response text.\n",
    "    \"\"\"\n",
    "    response = model_objects[model_name].prompt(prompt)\n",
    "    return response.text()\n",
    "\n",
    "def query_model_all(df, model_name):\n",
    "    \"\"\"\n",
    "    Query the chosen model for all prompts in the DataFrame and save responses.\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    cleaned_prompts = df[\"prompt\"].str.strip().str.lower()\n",
    "    colname = f\"response_{model_name}\"\n",
    "    df[colname] = cleaned_prompts.map(lambda x: query_model(x, model_name))\n",
    "    print(f\"{model_name} processing time: {time.time() - t0:.2f}s\")\n",
    "\n",
    "    # Ensure the 'responses' directory exists\n",
    "    os.makedirs(\"responses\", exist_ok=True)\n",
    "    \n",
    "    # Save responses for this model\n",
    "    response_file_path = f\"responses/responses_{model_name}.csv\"\n",
    "    df[[colname]].to_csv(response_file_path, index=False)\n",
    "    print(f\"Saved responses for {model_name} to {response_file_path}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def gather_all_model_responses(raw_prompts):\n",
    "    \"\"\"\n",
    "    Gather responses from all models and save to disk incrementally.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"prompt\": raw_prompts})\n",
    "    for m in MODEL_NAMES:\n",
    "        df = query_model_all(df, m)\n",
    "    return df\n",
    "\n",
    "##############################################################################\n",
    "# 3. Evaluate the responses\n",
    "##############################################################################\n",
    "\n",
    "def create_evaluation_mapping(other_models):\n",
    "    \"\"\"\n",
    "    Creates a mapping between model names and anonymous identifiers.\n",
    "    Returns both forward and reverse mappings.\n",
    "    \"\"\"\n",
    "    model_to_anonymous = {model: f\"Model_{i+1}\" for i, model in enumerate(other_models)}\n",
    "    anonymous_to_model = {v: k for k, v in model_to_anonymous.items()}\n",
    "    return model_to_anonymous, anonymous_to_model\n",
    "\n",
    "def build_evaluation_prompt(method, prompt, judge_model, model_responses, other_models):\n",
    "    \"\"\"\n",
    "    Build a meta-prompt with stronger emphasis on format requirements.\n",
    "    \"\"\"\n",
    "    model_to_anonymous, _ = create_evaluation_mapping(other_models)\n",
    "    \n",
    "    answers_section = \"\\n\".join([\n",
    "        f\"{model_to_anonymous[om]}:\\n{model_responses[om]}\\n---\" \n",
    "        for om in other_models\n",
    "    ])\n",
    "\n",
    "    if method == 1:\n",
    "        instructions = f\"\"\"IMPORTANT: You must ONLY return a JSON object with ratings. No explanation or additional text.\n",
    "\n",
    "PROMPT TO EVALUATE:\n",
    "\"{prompt}\"\n",
    "\n",
    "RESPONSES TO RATE:\n",
    "{answers_section}\n",
    "\n",
    "RATING INSTRUCTIONS:\n",
    "- Rate each response from 1 to 10\n",
    "- Consider: accuracy, completeness, clarity, relevance, depth, usefulness\n",
    "- 10: Exceptional, 8-9: Excellent, 6-7: Good, 4-5: Fair, 1-3: Poor\n",
    "\n",
    "YOUR RESPONSE MUST BE EXACTLY IN THIS FORMAT:\n",
    "{{\n",
    "    \"Model_1\": 8,\n",
    "    \"Model_2\": 7\n",
    "}}\n",
    "\n",
    "DO NOT include any other text, explanations, or analysis. ONLY the JSON object above.\"\"\"\n",
    "    else:\n",
    "        instructions = f\"\"\"IMPORTANT: You must ONLY return a JSON object with votes. No explanation or additional text.\n",
    "\n",
    "PROMPT TO EVALUATE:\n",
    "\"{prompt}\"\n",
    "\n",
    "RESPONSES TO RATE:\n",
    "{answers_section}\n",
    "\n",
    "VOTING INSTRUCTIONS:\n",
    "- \"Upvote\": helpful, accurate, effective response\n",
    "- \"Downvote\": inadequate, incorrect, or poor response\n",
    "\n",
    "YOUR RESPONSE MUST BE EXACTLY IN THIS FORMAT:\n",
    "{{\n",
    "    \"Model_1\": \"Upvote\",\n",
    "    \"Model_2\": \"Downvote\"\n",
    "}}\n",
    "\n",
    "DO NOT include any other text, explanations, or analysis. ONLY the JSON object above.\"\"\"\n",
    "\n",
    "    return instructions.strip(), model_to_anonymous\n",
    "\n",
    "def parse_evaluation_output(method, raw_judgment, anonymous_mapping):\n",
    "    \"\"\"\n",
    "    Enhanced parsing with better error handling and logging.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clean the input text more aggressively\n",
    "        cleaned_text = raw_judgment.strip()\n",
    "        # Find the first { and last }\n",
    "        start = cleaned_text.find(\"{\")\n",
    "        end = cleaned_text.rfind(\"}\") + 1\n",
    "        \n",
    "        if start == -1 or end == 0:\n",
    "            raise ValueError(\"No JSON object found in response\")\n",
    "            \n",
    "        json_str = cleaned_text[start:end]\n",
    "        data = json.loads(json_str)\n",
    "        \n",
    "    except (json.JSONDecodeError, ValueError) as e:\n",
    "        print(f\"Warning: Failed to parse judgment. Error: {str(e)}\")\n",
    "        print(f\"Raw response: {raw_judgment[:200]}...\")\n",
    "        # Return neutral fallback values\n",
    "        return {anonymous_mapping[k]: (5.0 if method == 1 else 0) for k in anonymous_mapping}\n",
    "\n",
    "    endorsement_map = {}\n",
    "    for anonymous_id, real_model in anonymous_mapping.items():\n",
    "        val = data.get(anonymous_id)\n",
    "        \n",
    "        if val is None:\n",
    "            print(f\"Warning: Missing rating for {anonymous_id}\")\n",
    "            endorsement_map[real_model] = 5.0 if method == 1 else 0\n",
    "            continue\n",
    "\n",
    "        if method == 1:\n",
    "            try:\n",
    "                score = float(val)\n",
    "                if not (1 <= score <= 10):\n",
    "                    print(f\"Warning: Score {score} for {anonymous_id} out of range, clamping to [1,10]\")\n",
    "                endorsement_map[real_model] = max(1.0, min(10.0, score))\n",
    "            except (TypeError, ValueError):\n",
    "                print(f\"Warning: Invalid numeric score for {anonymous_id}: {val}\")\n",
    "                endorsement_map[real_model] = 5.0\n",
    "        else:\n",
    "            if isinstance(val, str):\n",
    "                val = val.lower().strip()\n",
    "                if val == \"upvote\":\n",
    "                    endorsement_map[real_model] = 1\n",
    "                elif val == \"downvote\":\n",
    "                    endorsement_map[real_model] = 0\n",
    "                else:\n",
    "                    print(f\"Warning: Invalid vote value for {anonymous_id}: {val}, expected 'Upvote' or 'Downvote'\")\n",
    "                    endorsement_map[real_model] = 0\n",
    "            else:\n",
    "                print(f\"Warning: Invalid vote type for {anonymous_id}: {type(val)}\")\n",
    "                endorsement_map[real_model] = 0\n",
    "\n",
    "    return endorsement_map\n",
    "\n",
    "def evaluate_responses(df):\n",
    "    \"\"\"\n",
    "    Evaluate responses with improved error handling and DataFrame operations.\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(MODEL_NAMES)\n",
    "\n",
    "    # Initialize DataFrame to store evaluations\n",
    "    evaluations_df = pd.DataFrame({\n",
    "        'prompt': [],\n",
    "        'judge_model': [],\n",
    "        'rated_model_anonymous': [],\n",
    "        'rated_model_real': [],\n",
    "        'rating': [],\n",
    "        'method': []\n",
    "    })\n",
    "\n",
    "    # Filter valid evaluations only\n",
    "    valid_evaluations = evaluations_df[evaluations_df['rating'].notnull()]\n",
    "    \n",
    "    if valid_evaluations.empty:\n",
    "        print(\"No valid evaluations found. Skipping PageRank calculation.\")\n",
    "        return G, []\n",
    "\n",
    "    # Iterate through prompts and evaluate responses\n",
    "    for idx, row in df.iterrows():\n",
    "        prompt = row[\"prompt\"]\n",
    "        model_responses = {m: row.get(f\"response_{m}\", \"No response\") for m in MODEL_NAMES}\n",
    "\n",
    "        for judge_model in MODEL_NAMES:\n",
    "            other_models = [m for m in MODEL_NAMES if m != judge_model]\n",
    "\n",
    "            # Use subset evaluation if enabled\n",
    "            if USE_SUBSET_EVALUATION and len(other_models) > EVALUATORS_SUBSET_SIZE:\n",
    "                other_models = random.sample(other_models, EVALUATORS_SUBSET_SIZE)\n",
    "\n",
    "            # Skip if no valid models to evaluate\n",
    "            if not other_models:\n",
    "                print(f\"Skipping evaluation for prompt: {prompt} (insufficient valid responses)\")\n",
    "                continue\n",
    "\n",
    "            # Build evaluation prompt\n",
    "            evaluation_prompt, model_to_anonymous = build_evaluation_prompt(\n",
    "                EVALUATION_METHOD, prompt, judge_model, model_responses, other_models\n",
    "            )\n",
    "            anonymous_to_model = {v: k for k, v in model_to_anonymous.items()}\n",
    "\n",
    "            # Query the judge model\n",
    "            raw_judgment = query_model(evaluation_prompt, judge_model)\n",
    "            parsed_judgments = parse_evaluation_output(\n",
    "                EVALUATION_METHOD, raw_judgment, anonymous_to_model\n",
    "            )\n",
    "\n",
    "            # Create a new DataFrame for this batch of evaluations\n",
    "            new_evaluations = []\n",
    "            for rated_model, endorsement_val in parsed_judgments.items():\n",
    "                anonymous_id = model_to_anonymous[rated_model]\n",
    "                new_evaluations.append({\n",
    "                    'prompt': prompt,\n",
    "                    'judge_model': judge_model,\n",
    "                    'rated_model_anonymous': anonymous_id,\n",
    "                    'rated_model_real': rated_model,\n",
    "                    'rating': endorsement_val,\n",
    "                    'method': EVALUATION_METHOD\n",
    "                })\n",
    "            \n",
    "            # Concatenate efficiently\n",
    "            if new_evaluations:\n",
    "                evaluations_df = pd.concat([\n",
    "                    evaluations_df, \n",
    "                    pd.DataFrame(new_evaluations)\n",
    "                ], ignore_index=True)\n",
    "\n",
    "            # Update graph with valid ratings\n",
    "            for rated_model, endorsement_val in parsed_judgments.items():\n",
    "                if G.has_edge(judge_model, rated_model):\n",
    "                    G[judge_model][rated_model][\"weight\"] += endorsement_val\n",
    "                else:\n",
    "                    G.add_edge(judge_model, rated_model, weight=endorsement_val)\n",
    "\n",
    "    # Compute PageRank only if the graph has valid edges\n",
    "    if len(G.edges) == 0:\n",
    "        print(\"Graph has no valid edges. Cannot compute PageRank.\")\n",
    "        return G, []\n",
    "\n",
    "    pagerank_scores = nx.pagerank(G, weight=\"weight\")\n",
    "    ranked_models = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Save all data\n",
    "    nx.write_gml(G, \"responses/endorsement_graph.gml\")\n",
    "    print(\"Saved endorsement graph to endorsement_graph.gml\")\n",
    "\n",
    "    evaluations_df.to_csv(\"responses/evaluations_with_mapping.csv\", index=False)\n",
    "    print(\"Saved detailed evaluations with mappings to evaluations_with_mapping.csv\")\n",
    "\n",
    "    with open(\"rankings.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"rankings\": ranked_models,\n",
    "            \"metadata\": {\n",
    "                \"evaluation_method\": EVALUATION_METHOD,\n",
    "                \"use_subset_evaluation\": USE_SUBSET_EVALUATION,\n",
    "                \"evaluators_subset_size\": EVALUATORS_SUBSET_SIZE,\n",
    "                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "        }, f, indent=4)\n",
    "    print(\"Saved rankings to rankings.json\")\n",
    "\n",
    "    return G, ranked_models\n",
    "\n",
    "##############################################################################\n",
    "# 4. Main\n",
    "##############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prompts_df = pd.read_csv(\"prompts.csv\")  # Assuming the CSV has a column named \"Questions\"\n",
    "    raw_prompts = prompts_df[\"Questions\"].tolist()\n",
    "    raw_prompts = [prompts_df[\"Questions\"].iloc[1]]  # Use only select prompts - for testing\n",
    "\n",
    "    # Gather responses for each model\n",
    "    df_responses = gather_all_model_responses(raw_prompts)\n",
    "\n",
    "    # Evaluate responses and compute rankings\n",
    "    G, ranked = evaluate_responses(df_responses)\n",
    "\n",
    "    print(\"\\n=== PageRank Scores ===\")\n",
    "    for model, score in ranked:\n",
    "        print(f\"{model}: {score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
