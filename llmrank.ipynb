{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup config\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING,  # default to WARNING\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"SlopRankLogger\")\n",
    "logger.setLevel(logging.INFO)  # Our SlopRank logs at INFO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Configuration (EvalConfig)\n",
    "\n",
    "@dataclass\n",
    "class EvalConfig:\n",
    "    \"\"\"Configuration for the evaluation system.\"\"\"\n",
    "    model_names: List[str]\n",
    "    evaluation_method: int  # e.g., 1 => numeric rating\n",
    "    use_subset_evaluation: bool\n",
    "    evaluators_subset_size: int\n",
    "    output_dir: Path\n",
    "    request_delay: float = 0.0  # adjustable delay between requests if needed\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        if self.evaluation_method not in {1, 2}:\n",
    "            raise ValueError(\"evaluation_method must be 1 or 2\")\n",
    "        if self.evaluators_subset_size >= len(self.model_names):\n",
    "            raise ValueError(\"evaluators_subset_size must be < number of models\")\n",
    "\n",
    "DEFAULT_CONFIG = EvalConfig(\n",
    "    model_names=[\n",
    "        \"gemini-2.0-flash-thinking-exp-1219\",\n",
    "        \"gemini-exp-1206\",\n",
    "        \"claude-3-5-sonnet-latest\",\n",
    "        \"o1-preview\",\n",
    "        \"gpt-4o\",\n",
    "        \"deepseek-chat\"\n",
    "    ],\n",
    "    evaluation_method=1,  # numeric\n",
    "    use_subset_evaluation=False,\n",
    "    evaluators_subset_size=3,\n",
    "    output_dir=Path(\"results\"),  # folder for CSV outputs\n",
    "    request_delay=0.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 14:22:29,563 - INFO - Reading prompts from prompts.xlsx ...\n",
      "2025-01-09 14:22:29,612 - INFO - Loaded 5 prompts from Excel.\n"
     ]
    }
   ],
   "source": [
    "# 3. Read prompts\n",
    "# We assume you have a local \"prompts.xlsx\" file with columns [\"Questions\", \"Answer_key\"].\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # if you have .env credentials\n",
    "\n",
    "logger.info(\"Reading prompts from prompts.xlsx ...\")\n",
    "prompts_df = pd.read_excel(\"prompts.xlsx\", sheet_name=0)\n",
    "prompts = prompts_df[\"Questions\"].tolist()\n",
    "\n",
    "# If \"Answer_key\" column exists, read it; otherwise fallback to None\n",
    "if \"Answer_key\" in prompts_df.columns:\n",
    "    answer_keys = prompts_df[\"Answer_key\"].tolist()\n",
    "else:\n",
    "    logger.warning(\"No Answer_key column found; using None.\")\n",
    "    answer_keys = [None]*len(prompts_df)\n",
    "\n",
    "prompt_pairs = list(zip(prompts, answer_keys))\n",
    "logger.info(f\"Loaded {len(prompt_pairs)} prompts from Excel.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Collecting the responses (with partial checks)\n",
    "\n",
    "def collect_responses(prompt_pairs: List[Tuple[str, str]], config: EvalConfig, llm_module) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Query each model with each prompt, skipping any (prompt, model) pairs\n",
    "    already found in the existing responses.csv. \n",
    "    Return the combined DataFrame: (prompt, model, response, is_valid, response_time, Answer_key, token_count).\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Collecting responses (with partial coverage check)...\")\n",
    "\n",
    "    # 1) Try to load existing responses\n",
    "    resp_path = config.output_dir / \"responses.csv\"\n",
    "    existing_responses_df = None\n",
    "    if resp_path.exists():\n",
    "        logger.info(f\"Found existing responses at {resp_path}, will skip duplicates.\")\n",
    "        existing_responses_df = pd.read_csv(resp_path)\n",
    "    else:\n",
    "        logger.info(\"No existing responses file found; we'll collect everything from scratch.\")\n",
    "\n",
    "    new_rows = []\n",
    "    total_start = time.time()\n",
    "\n",
    "    # 2) For each (prompt, answer_key) pair\n",
    "    for i, (prompt, answer_key) in enumerate(prompt_pairs, 1):\n",
    "        logger.info(f\"Processing prompt {i}/{len(prompt_pairs)}: {prompt[:60]}...\")\n",
    "        for model_name in config.model_names:\n",
    "            # Skip if we already have a row for (prompt, model_name)\n",
    "            if existing_responses_df is not None:\n",
    "                subset = existing_responses_df[\n",
    "                    (existing_responses_df[\"prompt\"] == prompt) &\n",
    "                    (existing_responses_df[\"model\"] == model_name)\n",
    "                ]\n",
    "                if not subset.empty:\n",
    "                    # Already have it; skip\n",
    "                    logger.info(f\"Skipping existing response for model={model_name}, prompt={prompt[:40]}...\")\n",
    "                    continue\n",
    "\n",
    "            # Otherwise, query the model now\n",
    "            start_time = time.time()\n",
    "            logger.info(f\"Querying {model_name} for new response...\")\n",
    "            try:\n",
    "                model = llm_module.get_model(model_name)\n",
    "                raw_response = model.prompt(prompt).text()\n",
    "\n",
    "                valid = isinstance(raw_response, str) and len(raw_response.strip()) >= 10\n",
    "                elapsed = time.time() - start_time\n",
    "                tokens_used = len(raw_response.split())\n",
    "\n",
    "                new_rows.append({\n",
    "                    'prompt': prompt,\n",
    "                    'model': model_name,\n",
    "                    'response': raw_response if valid else None,\n",
    "                    'is_valid': valid,\n",
    "                    'response_time': elapsed,\n",
    "                    'Answer_key': answer_key,\n",
    "                    'token_count': tokens_used\n",
    "                })\n",
    "                logger.info(\n",
    "                    f\"{model_name} responded in {elapsed:.2f}s - {'Valid' if valid else 'Invalid'}\"\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                elapsed = time.time() - start_time\n",
    "                logger.error(f\"Error from {model_name} after {elapsed:.2f}s: {str(e)}\")\n",
    "                new_rows.append({\n",
    "                    'prompt': prompt,\n",
    "                    'model': model_name,\n",
    "                    'response': None,\n",
    "                    'is_valid': False,\n",
    "                    'response_time': elapsed,\n",
    "                    'Answer_key': answer_key,\n",
    "                    'token_count': 0\n",
    "                })\n",
    "\n",
    "            if config.request_delay > 0.0:\n",
    "                time.sleep(config.request_delay)\n",
    "\n",
    "    total_time = time.time() - total_start\n",
    "    logger.info(f\"Response collection done in {total_time:.2f}s\")\n",
    "\n",
    "    # 3) Combine with existing responses if any\n",
    "    if existing_responses_df is not None:\n",
    "        new_df = pd.DataFrame(new_rows)\n",
    "        combined_df = pd.concat([existing_responses_df, new_df], ignore_index=True)\n",
    "        # Drop duplicates if needed\n",
    "        combined_df.drop_duplicates(subset=[\"prompt\", \"model\"], keep=\"first\", inplace=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        # No prior file => just return new rows\n",
    "        return pd.DataFrame(new_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Collecting Raw Evaluations (Unparsed), with partial checks\n",
    "\n",
    "def collect_raw_evaluations(responses_df: pd.DataFrame, config: EvalConfig, llm_module) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Each model in config.model_names evaluates (rates) the others' responses\n",
    "    but we skip if we already have a row for (prompt, judge_model, model_mapping) \n",
    "    in raw_evaluations.csv. \n",
    "    Returns the combined DataFrame of new + old.\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Collecting raw evaluations (unparsed, partial check)...\")\n",
    "\n",
    "    # 1) Try loading existing raw evaluations\n",
    "    raw_eval_path = config.output_dir / \"raw_evaluations.csv\"\n",
    "    existing_raw_eval_df = None\n",
    "    if raw_eval_path.exists():\n",
    "        logger.info(f\"Found existing raw evaluations at {raw_eval_path}, will skip duplicates.\")\n",
    "        existing_raw_eval_df = pd.read_csv(raw_eval_path)\n",
    "    else:\n",
    "        logger.info(\"No existing raw_evaluations.csv found; collecting from scratch.\")\n",
    "\n",
    "    new_judgments = []\n",
    "\n",
    "    unique_prompts = responses_df['prompt'].unique()\n",
    "    for prompt in unique_prompts:\n",
    "        prompt_subset = responses_df[responses_df['prompt'] == prompt]\n",
    "        answer_key = prompt_subset['Answer_key'].iloc[0] if 'Answer_key' in prompt_subset.columns else None\n",
    "        prompt_responses = prompt_subset.set_index('model')['response'].to_dict()\n",
    "\n",
    "        # Evaluate with each model as judge\n",
    "        for judge_model in config.model_names:\n",
    "            # Exclude judge's own or missing responses\n",
    "            other_models = [\n",
    "                m for m in config.model_names\n",
    "                if m != judge_model and prompt_responses.get(m) is not None\n",
    "            ]\n",
    "            if config.use_subset_evaluation and other_models:\n",
    "                other_models = random.sample(\n",
    "                    other_models,\n",
    "                    min(config.evaluators_subset_size, len(other_models))\n",
    "                )\n",
    "\n",
    "            if not other_models:\n",
    "                continue\n",
    "\n",
    "            # Build the anonymized mapping\n",
    "            model_to_anon = {m: f\"Model_{i+1}\" for i, m in enumerate(other_models)}\n",
    "            answers_section = \"\\n\".join([\n",
    "                f\"{model_to_anon[m]}:\\n{prompt_responses[m]}\\n---\"\n",
    "                for m in other_models\n",
    "            ])\n",
    "            if answer_key:\n",
    "                answer_key_edited = f\"The Answer Key here is:\\n{answer_key}\\n---\\n\"\n",
    "            else:\n",
    "                answer_key_edited = \"\"\n",
    "\n",
    "            instructions = f\"\"\"\n",
    "You are an expert evaluator tasked with assessing the quality of responses from different language models. Your goal is to provide accurate and unbiased ratings based on a given problem, answer key, and set of criteria.\n",
    "\n",
    "Here is the original problem or prompt:\n",
    "<problem>\n",
    "{prompt}\n",
    "</problem>\n",
    "\n",
    "Here are the answers provided by different models:\n",
    "<answers_section>\n",
    "{answers_section}\n",
    "</answers_section>\n",
    "\n",
    "Here is the answer key to guide your evaluation:\n",
    "<answer_key>\n",
    "{answer_key_edited}\n",
    "</answer_key>\n",
    "\n",
    "Your task is to evaluate the answers provided by Model_1 and Model_2 based on the following criteria:\n",
    "1. Accuracy: How well does the answer align with the information in the answer key?\n",
    "2. Completeness: Does the answer cover all necessary aspects of the problem?\n",
    "3. Clarity: Is the answer easy to understand and well-structured?\n",
    "4. Relevance: Does the answer directly address the given problem?\n",
    "\n",
    "For each model, you will provide a rating on a scale of 1 to 10 for each criterion, where:\n",
    "- 10: Exceptional, world-leading class\n",
    "- 8-9: Excellent, like a top professional in the field\n",
    "- 6-7: Good, like a competent undergraduate student\n",
    "- 4-5: Fair, like an average high school student\n",
    "- 1-3: Poor\n",
    "\n",
    "Please follow the following process to evaluate each model:\n",
    "\n",
    "1. Read the problem, answer key, and the model's answer carefully.\n",
    "2. For each criterion:\n",
    "   a. Write down key points from the answer that relate to this criterion.\n",
    "   b. Consider both strengths and weaknesses.\n",
    "   c. Provide your reasoning and a score.\n",
    "3. Calculate an overall score based on the individual criterion scores.\n",
    "4. Format the final rating as a JSON object.\n",
    "\n",
    "Wrap your detailed evaluation for each model in <detailed_evaluation> tags.\n",
    "\n",
    "After evaluating both models, provide your final ratings in a JSON object with the following structure:\n",
    "{{\"Model_1\": X, \"Model_2\": Y}}\n",
    "\n",
    "Where X and Y are integer values between 1 and 10.\n",
    "\n",
    "Remember:\n",
    "- Adhere strictly to the JSON format specified above, i.e., put the response inside a curly bracket.\n",
    "- Provide neutral and accurate ratings based solely on the answer key and the given criteria.\n",
    "- Ensure that your evaluation is thorough and justified.\n",
    "\n",
    "Begin your evaluation now.\"\"\".strip()\n",
    "\n",
    "            # 2) If we already have a row for (prompt, judge_model, model_mapping), skip\n",
    "            model_mapping_str = json.dumps(model_to_anon, sort_keys=True)\n",
    "            already_exists = False\n",
    "            if existing_raw_eval_df is not None:\n",
    "                possible_matches = existing_raw_eval_df[\n",
    "                    (existing_raw_eval_df[\"prompt\"] == prompt) &\n",
    "                    (existing_raw_eval_df[\"judge_model\"] == judge_model)\n",
    "                ]\n",
    "                # Now check if any row has the exact same model_mapping\n",
    "                # We sort_keys=True above so that JSON string is consistent\n",
    "                found_match = possible_matches[\n",
    "                    possible_matches[\"model_mapping\"] == model_mapping_str\n",
    "                ]\n",
    "                if not found_match.empty:\n",
    "                    logger.info(f\"Skipping existing raw eval for judge={judge_model}, prompt={prompt[:40]}...\")\n",
    "                    already_exists = True\n",
    "\n",
    "            if already_exists:\n",
    "                continue\n",
    "\n",
    "            # 3) Otherwise, run the LLM judge\n",
    "            try:\n",
    "                judge_llm = llm_module.get_model(judge_model)\n",
    "                judge_result_obj = judge_llm.prompt(instructions)\n",
    "                raw_judgment = judge_result_obj.text()\n",
    "                raw_judgment_tokens = len(raw_judgment.split())\n",
    "\n",
    "                new_judgments.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"judge_model\": judge_model,\n",
    "                    \"raw_judgment\": raw_judgment,\n",
    "                    # store the same sorted string\n",
    "                    \"model_mapping\": model_mapping_str,\n",
    "                    \"raw_judgment_token_count\": raw_judgment_tokens\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error collecting raw eval from judge={judge_model} on prompt='{prompt}': {str(e)}\")\n",
    "                new_judgments.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"judge_model\": judge_model,\n",
    "                    \"raw_judgment\": None,\n",
    "                    \"model_mapping\": model_mapping_str,\n",
    "                    \"raw_judgment_token_count\": 0,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "\n",
    "    new_eval_df = pd.DataFrame(new_judgments)\n",
    "    logger.info(\"Finished collecting new raw evaluation outputs.\")\n",
    "\n",
    "    # 4) Combine with existing, if any\n",
    "    if existing_raw_eval_df is not None and not new_eval_df.empty:\n",
    "        combined_df = pd.concat([existing_raw_eval_df, new_eval_df], ignore_index=True)\n",
    "        combined_df.drop_duplicates(subset=[\"prompt\", \"judge_model\", \"model_mapping\"], keep=\"first\", inplace=True)\n",
    "        return combined_df\n",
    "    elif existing_raw_eval_df is not None:\n",
    "        # No new data, just return old\n",
    "        return existing_raw_eval_df\n",
    "    else:\n",
    "        # Everything is new\n",
    "        return new_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Parsing the raw evaluations\n",
    "\n",
    "def parse_evaluation_rows(raw_eval_df: pd.DataFrame, config: EvalConfig) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse each row of the raw_eval_df, which contains judge's raw JSON-like output.\n",
    "    If parsing fails, fallback to a default rating (4.1) for each rated model.\n",
    "    \n",
    "    Returns a DataFrame: (prompt, judge_model, rated_model, score).\n",
    "    \"\"\"\n",
    "    evaluations = []\n",
    "\n",
    "    for _, row in raw_eval_df.iterrows():\n",
    "        prompt = row[\"prompt\"]\n",
    "        judge_model = row[\"judge_model\"]\n",
    "        raw_judgment = row[\"raw_judgment\"]\n",
    "        raw_judgment_tokens = row.get(\"raw_judgment_token_count\", 0)\n",
    "\n",
    "        # Convert model_mapping from JSON string back to dict\n",
    "        try:\n",
    "            model_mapping = json.loads(row[\"model_mapping\"])  # e.g. {\"gemini-exp-1206\":\"Model_1\"}\n",
    "        except:\n",
    "            model_mapping = {}\n",
    "\n",
    "        if not raw_judgment:\n",
    "            # If there's no raw judgment at all, we might skip or fallback\n",
    "            for real_model in model_mapping.keys():\n",
    "                evaluations.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"judge_model\": judge_model,\n",
    "                    \"rated_model\": real_model,\n",
    "                    \"score\": 4.1,           # << changed fallback\n",
    "                    \"parse_failed\": True\n",
    "                })\n",
    "            logger.warning(f\"No raw_judgment for prompt={prompt}, judge={judge_model}; skipping parse.\")\n",
    "            continue\n",
    "\n",
    "        # Try to parse a JSON object from the raw_judgment\n",
    "        try:\n",
    "            start = raw_judgment.find(\"{\")\n",
    "            end = raw_judgment.rfind(\"}\") + 1\n",
    "\n",
    "            if start == -1 or end == 0:\n",
    "                raise ValueError(\"No JSON object found in raw_judgment\")\n",
    "\n",
    "            data = json.loads(raw_judgment[start:end])\n",
    "            # Reverse mapping: \"Model_1\" => \"gemini-exp-1206\"\n",
    "            anon_to_real = {v: k for k, v in model_mapping.items()}\n",
    "\n",
    "            for anon_id, score in data.items():\n",
    "                real_model = anon_to_real.get(anon_id)\n",
    "                if not real_model:\n",
    "                    # If we can't find the real model name, skip\n",
    "                    continue\n",
    "                numeric_score = float(score)\n",
    "                numeric_score = max(1.0, min(10.0, numeric_score))  # clamp 1..10\n",
    "\n",
    "                evaluations.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"judge_model\": judge_model,\n",
    "                    \"rated_model\": real_model,\n",
    "                    \"score\": numeric_score,\n",
    "                    \"parse_failed\": False,\n",
    "                    \"raw_judgment_token_count\": raw_judgment_tokens\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Parsing error for judge={judge_model}, prompt={prompt}: {str(e)}\")\n",
    "            # If parse fails, assign a default rating\n",
    "            for real_model in model_mapping.keys():\n",
    "                evaluations.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"judge_model\": judge_model,\n",
    "                    \"rated_model\": real_model,\n",
    "                    \"score\": 4.1,\n",
    "                    \"parse_failed\": True,\n",
    "                    \"raw_judgment_token_count\": raw_judgment_tokens\n",
    "                })\n",
    "\n",
    "    evals_df = pd.DataFrame(evaluations)\n",
    "    return evals_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 14:22:57,267 - INFO - Using config: EvalConfig(model_names=['gemini-2.0-flash-thinking-exp-1219', 'gemini-exp-1206', 'claude-3-5-sonnet-latest', 'o1-preview', 'gpt-4o', 'deepseek-chat'], evaluation_method=1, use_subset_evaluation=False, evaluators_subset_size=3, output_dir=PosixPath('results'), request_delay=0.0)\n",
      "2025-01-09 14:22:57,269 - INFO - Loading existing responses from results/responses.csv\n",
      "2025-01-09 14:22:57,294 - INFO - No raw_evaluations.csv found; collecting now (unparsed).\n",
      "2025-01-09 14:22:57,294 - INFO - Collecting raw evaluations (unparsed)...\n",
      "2025-01-09 14:49:01,107 - INFO - Finished collecting raw evaluation outputs.\n",
      "2025-01-09 14:49:01,115 - INFO - Saved raw evaluations to results/raw_evaluations.csv\n",
      "2025-01-09 14:49:01,116 - INFO - Loading parsed evaluations from results/evaluations.csv\n",
      "2025-01-09 14:49:01,122 - INFO - Here are the first few rows of the parsed evaluations:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>judge_model</th>\n",
       "      <th>rated_model</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Provide a summary of the gene TM2D2, including...</td>\n",
       "      <td>gemini-2.0-flash-thinking-exp-1219</td>\n",
       "      <td>deepseek-chat</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Provide a summary of the gene TM2D2, including...</td>\n",
       "      <td>gemini-2.0-flash-thinking-exp-1219</td>\n",
       "      <td>o1-preview</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Provide a summary of the gene TM2D2, including...</td>\n",
       "      <td>gemini-2.0-flash-thinking-exp-1219</td>\n",
       "      <td>gemini-exp-1206</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Provide a summary of the gene TM2D2, including...</td>\n",
       "      <td>gemini-exp-1206</td>\n",
       "      <td>o1-preview</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Provide a summary of the gene TM2D2, including...</td>\n",
       "      <td>gemini-exp-1206</td>\n",
       "      <td>gemini-2.0-flash-thinking-exp-1219</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Provide a summary of the gene TM2D2, including...   \n",
       "1  Provide a summary of the gene TM2D2, including...   \n",
       "2  Provide a summary of the gene TM2D2, including...   \n",
       "3  Provide a summary of the gene TM2D2, including...   \n",
       "4  Provide a summary of the gene TM2D2, including...   \n",
       "\n",
       "                          judge_model                         rated_model  \\\n",
       "0  gemini-2.0-flash-thinking-exp-1219                       deepseek-chat   \n",
       "1  gemini-2.0-flash-thinking-exp-1219                          o1-preview   \n",
       "2  gemini-2.0-flash-thinking-exp-1219                     gemini-exp-1206   \n",
       "3                     gemini-exp-1206                          o1-preview   \n",
       "4                     gemini-exp-1206  gemini-2.0-flash-thinking-exp-1219   \n",
       "\n",
       "   score  \n",
       "0    5.0  \n",
       "1    5.0  \n",
       "2    5.0  \n",
       "3    8.0  \n",
       "4    7.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 7. Full workflow\n",
    "import llm  # custom LLM module\n",
    "\n",
    "# 1) Create a config\n",
    "config = DEFAULT_CONFIG\n",
    "logger.info(f\"Using config: {config}\")\n",
    "\n",
    "# 2) Collect or load responses\n",
    "resp_path = config.output_dir / \"responses.csv\"\n",
    "\n",
    "if resp_path.exists():\n",
    "    logger.info(f\"Loading existing responses from {resp_path}\")\n",
    "    responses_df = pd.read_csv(resp_path)\n",
    "else:\n",
    "    logger.info(\"No responses.csv found; collecting now from each model.\")\n",
    "    responses_df = collect_responses(prompt_pairs, config, llm)\n",
    "    responses_df.to_csv(resp_path, index=False)\n",
    "    logger.info(f\"Saved new responses to {resp_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Collect or load raw evaluations\n",
    "raw_eval_path = config.output_dir / \"raw_evaluations.csv\"\n",
    "\n",
    "if raw_eval_path.exists():\n",
    "    logger.info(f\"Loading existing raw evaluations from {raw_eval_path}\")\n",
    "    raw_eval_df = pd.read_csv(raw_eval_path)\n",
    "else:\n",
    "    logger.info(\"No raw_evaluations.csv found; collecting now (unparsed).\")\n",
    "    raw_eval_df = collect_raw_evaluations(responses_df, config, llm)\n",
    "    raw_eval_df.to_csv(raw_eval_path, index=False)\n",
    "    logger.info(f\"Saved raw evaluations to {raw_eval_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Parse or load final evaluations\n",
    "eval_path = config.output_dir / \"evaluations.csv\"\n",
    "\n",
    "if eval_path.exists():\n",
    "    logger.info(f\"Loading parsed evaluations from {eval_path}\")\n",
    "    evaluations_df = pd.read_csv(eval_path)\n",
    "else:\n",
    "    logger.info(\"No evaluations.csv found; parsing raw evaluations now.\")\n",
    "    evaluations_df = parse_evaluation_rows(raw_eval_df, config)\n",
    "    evaluations_df.to_csv(eval_path, index=False)\n",
    "    logger.info(f\"Saved parsed evaluations to {eval_path}\")\n",
    "\n",
    "\n",
    "# 10. Inspect or analyze the final numeric scores\n",
    "logger.info(\"Here are the first few rows of the parsed evaluations:\")\n",
    "display(evaluations_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
