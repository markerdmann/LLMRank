{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SlopRank\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING,  # default to WARNING\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"SlopRankLogger\")\n",
    "logger.setLevel(logging.INFO)  # SlopRank logs at INFO\n",
    "\n",
    "@dataclass\n",
    "class EvalConfig:\n",
    "    \"\"\"Configuration for the evaluation system.\"\"\"\n",
    "    model_names: List[str]\n",
    "    evaluation_method: int  # 1 for numeric, 2 for ranking\n",
    "    use_subset_evaluation: bool\n",
    "    evaluators_subset_size: int\n",
    "    output_dir: Path\n",
    "    request_delay: float = 0.0  # adjustable delay between requests if needed\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        if self.evaluation_method not in {1, 2}:\n",
    "            raise ValueError(\"evaluation_method must be 1 or 2\")\n",
    "        if self.evaluators_subset_size >= len(self.model_names):\n",
    "            raise ValueError(\"evaluators_subset_size must be less than number of models\")\n",
    "\n",
    "DEFAULT_CONFIG = EvalConfig(\n",
    "    model_names=[\n",
    "        \"gemini-2.0-flash-thinking-exp-1219\",\n",
    "        \"gemini-exp-1206\",\n",
    "        \"claude-3-5-sonnet-latest\",\n",
    "        \"o1-preview\",\n",
    "        \"gpt-4o\",\n",
    "        \"deepseek-chat\"\n",
    "    ],\n",
    "    evaluation_method=1,\n",
    "    use_subset_evaluation=True,\n",
    "    evaluators_subset_size=3,\n",
    "    output_dir=Path(\"results\"),\n",
    "    request_delay=0.5\n",
    ")\n",
    "\n",
    "class SlopRank:\n",
    "    \"\"\"\n",
    "    Main class for running the evaluation pipeline:\n",
    "    1. Collect responses from all models.\n",
    "    2. Evaluate them with subset or full evaluations.\n",
    "    3. Build a graph from the judgments.\n",
    "    4. Run PageRank to rank the models.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: EvalConfig = DEFAULT_CONFIG):\n",
    "        self.config = config\n",
    "        self.responses_df = None\n",
    "        self.evaluations_df = None\n",
    "\n",
    "    def _validate_response(self, response: str) -> bool:\n",
    "        \"\"\"Basic validation of model responses.\"\"\"\n",
    "        if not isinstance(response, str):\n",
    "            return False\n",
    "        return len(response.strip()) >= 10\n",
    "\n",
    "    def collect_responses(self, prompts: List[str], llm_module) -> pd.DataFrame:\n",
    "        \"\"\"Collect responses from all models for given prompts.\"\"\"\n",
    "        logger.info(\"Collecting responses...\")\n",
    "        responses = []\n",
    "        total_start = time.time()\n",
    "\n",
    "        for i, prompt in enumerate(prompts, 1):\n",
    "            logger.info(f\"Processing prompt {i}/{len(prompts)}\")\n",
    "            for model_name in self.config.model_names:\n",
    "                start_time = time.time()\n",
    "                logger.info(f\"Querying {model_name}...\")\n",
    "                try:\n",
    "                    model = llm_module.get_model(model_name)\n",
    "                    response = model.prompt(prompt).text()\n",
    "                    valid = self._validate_response(response)\n",
    "                    elapsed = time.time() - start_time\n",
    "\n",
    "                    responses.append({\n",
    "                        'prompt': prompt,\n",
    "                        'model': model_name,\n",
    "                        'response': response if valid else None,\n",
    "                        'is_valid': valid,\n",
    "                        'response_time': elapsed\n",
    "                    })\n",
    "                    logger.info(f\"{model_name} responded in {elapsed:.2f}s - \"\n",
    "                                f\"{'Valid' if valid else 'Invalid'}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    logger.error(f\"Error from {model_name} after {elapsed:.2f}s: {str(e)}\")\n",
    "                    responses.append({\n",
    "                        'prompt': prompt,\n",
    "                        'model': model_name,\n",
    "                        'response': None,\n",
    "                        'is_valid': False,\n",
    "                        'response_time': elapsed\n",
    "                    })\n",
    "\n",
    "                # Respect optional delay to avoid rate limits\n",
    "                if self.config.request_delay > 0.0:\n",
    "                    time.sleep(self.config.request_delay)\n",
    "\n",
    "        total_time = time.time() - total_start\n",
    "        logger.info(f\"All responses collected in {total_time:.2f}s\")\n",
    "        self.responses_df = pd.DataFrame(responses)\n",
    "        return self.responses_df\n",
    "\n",
    "    def _create_evaluation_prompt(self, prompt: str, responses: Dict[str, str]) -> Tuple[str, Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Creates the evaluation prompt, anonymizing the model names,\n",
    "        and returns the mapping as well.\n",
    "        \"\"\"\n",
    "        model_to_anon = {m: f\"Model_{i+1}\" for i, m in enumerate(responses.keys())}\n",
    "        answers_section = \"\\n\".join([\n",
    "            f\"{model_to_anon[m]}:\\n{resp}\\n---\"\n",
    "            for m, resp in responses.items()\n",
    "        ])\n",
    "\n",
    "        if self.config.evaluation_method == 1:\n",
    "            instructions = f\"\"\"IMPORTANT: Return only a JSON object with ratings.\n",
    "\n",
    "            Rate these responses to: \"{prompt}\"\n",
    "\n",
    "            {answers_section}\n",
    "\n",
    "            Rate 1-10 based on: accuracy, completeness, clarity, relevance, depth, usefulness\n",
    "            10: Exceptional, 8-9: Excellent, 6-7: Good, 4-5: Fair, 1-3: Poor\n",
    "\n",
    "            Format: {{\"Model_1\": 8, \"Model_2\": 7}}\n",
    "            \"\"\"\n",
    "        else:\n",
    "            instructions = f\"\"\"IMPORTANT: Return only a JSON object with rankings.\n",
    "\n",
    "            Rank these responses to: \"{prompt}\"\n",
    "\n",
    "            {answers_section}\n",
    "\n",
    "            Rank from best (1) to worst. No ties allowed.\n",
    "            Consider: accuracy, completeness, clarity, relevance, depth, usefulness\n",
    "\n",
    "            Format: {{\"Model_1\": 1, \"Model_2\": 2}}\n",
    "            \"\"\"\n",
    "\n",
    "        return instructions.strip(), model_to_anon\n",
    "\n",
    "    def _parse_evaluation(self, raw_judgment: str, model_mapping: Dict[str, str]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Parse and validate evaluation responses. If parsing fails,\n",
    "        fallback to neutral or default scores.\n",
    "        \"\"\"\n",
    "        # Try to find the JSON object and parse it\n",
    "        try:\n",
    "            start = raw_judgment.find(\"{\")\n",
    "            end = raw_judgment.rfind(\"}\") + 1\n",
    "            if start == -1 or end == 0:\n",
    "                raise ValueError(\"No JSON object found.\")\n",
    "\n",
    "            data = json.loads(raw_judgment[start:end])\n",
    "            anon_to_model = {v: k for k, v in model_mapping.items()}\n",
    "\n",
    "            results = {}\n",
    "            for anon_id, score in data.items():\n",
    "                model_name = anon_to_model.get(anon_id)\n",
    "                if not model_name:\n",
    "                    continue\n",
    "\n",
    "                if self.config.evaluation_method == 1:\n",
    "                    # Numeric scores, clamp between 1.0 and 10.0\n",
    "                    numeric_score = float(score)\n",
    "                    numeric_score = max(1.0, min(10.0, numeric_score))\n",
    "                    results[model_name] = numeric_score\n",
    "                else:\n",
    "                    # Rankings must be integers\n",
    "                    rank_score = int(score)\n",
    "                    results[model_name] = rank_score\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing evaluation: {str(e)}\")\n",
    "            # Fallback to default\n",
    "            if self.config.evaluation_method == 1:\n",
    "                # default score 5.0 for numeric\n",
    "                return {m: 5.0 for m in model_mapping.keys()}\n",
    "            else:\n",
    "                # default rank = \"number of items\" for ranking\n",
    "                # so everything is equally \"worst\"\n",
    "                return {\n",
    "                    m: len(model_mapping)\n",
    "                    for m in model_mapping.keys()\n",
    "                }\n",
    "\n",
    "    def evaluate_responses(self, responses_df: pd.DataFrame, llm_module) -> Tuple[nx.DiGraph, pd.DataFrame]:\n",
    "        \"\"\"Evaluate all responses and build the graph.\"\"\"\n",
    "        logger.info(\"Evaluating responses...\")\n",
    "        G = nx.DiGraph()\n",
    "        G.add_nodes_from(self.config.model_names)\n",
    "        evaluations = []\n",
    "\n",
    "        # For each unique prompt\n",
    "        for prompt in responses_df['prompt'].unique():\n",
    "            prompt_subset = responses_df[responses_df['prompt'] == prompt]\n",
    "            prompt_responses = prompt_subset.set_index('model')['response'].to_dict()\n",
    "\n",
    "            # For each model that will serve as the judge\n",
    "            for judge_model in self.config.model_names:\n",
    "                # Filter out the judge from rated models and ignore invalid or missing responses\n",
    "                other_models = [\n",
    "                    m for m in self.config.model_names\n",
    "                    if m != judge_model and prompt_responses.get(m) is not None\n",
    "                ]\n",
    "\n",
    "                if self.config.use_subset_evaluation:\n",
    "                    other_models = random.sample(\n",
    "                        other_models,\n",
    "                        min(self.config.evaluators_subset_size, len(other_models))\n",
    "                    )\n",
    "\n",
    "                if not other_models:\n",
    "                    continue\n",
    "\n",
    "                # Build the evaluation prompt\n",
    "                eval_prompt, model_mapping = self._create_evaluation_prompt(\n",
    "                    prompt,\n",
    "                    {m: prompt_responses[m] for m in other_models}\n",
    "                )\n",
    "\n",
    "                try:\n",
    "                    raw_judgment = llm_module.get_model(judge_model).prompt(eval_prompt).text()\n",
    "                    parsed_judgments = self._parse_evaluation(raw_judgment, model_mapping)\n",
    "\n",
    "                    # Record judgments\n",
    "                    for rated_model, score in parsed_judgments.items():\n",
    "                        evaluations.append({\n",
    "                            'prompt': prompt,\n",
    "                            'judge_model': judge_model,\n",
    "                            'rated_model': rated_model,\n",
    "                            'score': score\n",
    "                        })\n",
    "                        # Update graph\n",
    "                        if G.has_edge(judge_model, rated_model):\n",
    "                            G[judge_model][rated_model]['weight'] += score\n",
    "                        else:\n",
    "                            G.add_edge(judge_model, rated_model, weight=score)\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error during evaluation by {judge_model}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "        return G, pd.DataFrame(evaluations)\n",
    "\n",
    "    def run(self, prompts: List[str], llm_module) -> Dict:\n",
    "        \"\"\"\n",
    "        Run the full SlopRank evaluation:\n",
    "         - Collect responses\n",
    "         - Evaluate them\n",
    "         - Compute PageRank\n",
    "         - Save results\n",
    "        \"\"\"\n",
    "        # 1. Collect responses\n",
    "        self.collect_responses(prompts, llm_module)\n",
    "        if self.responses_df is None or self.responses_df.empty:\n",
    "            logger.error(\"No responses collected, aborting.\")\n",
    "            return {}\n",
    "\n",
    "        # 2. Save responses\n",
    "        resp_path = self.config.output_dir / \"responses.csv\"\n",
    "        self.responses_df.to_csv(resp_path, index=False)\n",
    "        logger.info(f\"Saved responses to {resp_path}\")\n",
    "\n",
    "        # 3. Evaluate\n",
    "        G, self.evaluations_df = self.evaluate_responses(self.responses_df, llm_module)\n",
    "        if self.evaluations_df.empty:\n",
    "            logger.error(\"No evaluations produced, aborting.\")\n",
    "            return {}\n",
    "\n",
    "        # 4. Save evaluations\n",
    "        eval_path = self.config.output_dir / \"evaluations.csv\"\n",
    "        self.evaluations_df.to_csv(eval_path, index=False)\n",
    "        logger.info(f\"Saved evaluations to {eval_path}\")\n",
    "\n",
    "        # 5. Compute PageRank\n",
    "        if len(G.edges) == 0:\n",
    "            logger.error(\"No valid edges to compute PageRank, aborting.\")\n",
    "            return {}\n",
    "\n",
    "        pagerank_scores = nx.pagerank(G, weight=\"weight\")\n",
    "        ranked_models = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # 6. Save graph\n",
    "        gml_path = self.config.output_dir / \"endorsement_graph.gml\"\n",
    "        nx.write_gml(G, gml_path)\n",
    "        logger.info(f\"Saved graph to {gml_path}\")\n",
    "\n",
    "        # 7. Save final results\n",
    "        results = {\n",
    "            \"rankings\": ranked_models,\n",
    "            \"metadata\": {\n",
    "                \"evaluation_method\": self.config.evaluation_method,\n",
    "                \"use_subset_evaluation\": self.config.use_subset_evaluation,\n",
    "                \"evaluators_subset_size\": self.config.evaluators_subset_size,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "        rankings_path = self.config.output_dir / \"rankings.json\"\n",
    "        with open(rankings_path, \"w\") as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "        logger.info(f\"Saved rankings to {rankings_path}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "def main():\n",
    "    import llm  # Your LLM module import\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "\n",
    "    slop_rank_logger = logging.getLogger(\"SlopRankLogger\")\n",
    "    slop_rank_logger.info(\"Starting SlopRank evaluation\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        # Read prompts\n",
    "        slop_rank_logger.info(\"Reading prompts from prompts.csv\")\n",
    "        prompts_df = pd.read_csv(\"prompts.csv\")\n",
    "        prompts = prompts_df[\"Questions\"].tolist()\n",
    "        slop_rank_logger.info(f\"Loaded {len(prompts)} prompts\")\n",
    "\n",
    "        # Initialize evaluation\n",
    "        config = DEFAULT_CONFIG\n",
    "        slop_rank_logger.info(f\"Using configuration: {config}\")\n",
    "        evaluator = SlopRank(config)\n",
    "\n",
    "        # Run evaluation\n",
    "        results = evaluator.run(prompts, llm)\n",
    "\n",
    "        if results:\n",
    "            print(\"\\n=== Model Rankings ===\")\n",
    "            max_score = max(score for _, score in results[\"rankings\"])\n",
    "            for model, score in results[\"rankings\"]:\n",
    "                normalized_score = (score / max_score) * 10  # Normalize to 0-10\n",
    "                print(f\"{model:30} {score:.4f} (normalized: {normalized_score:.2f})\")\n",
    "\n",
    "            total_time = time.time() - start_time\n",
    "            slop_rank_logger.info(f\"Evaluation completed in {total_time:.2f}s\")\n",
    "        else:\n",
    "            slop_rank_logger.error(\"No results generated\")\n",
    "    except Exception as e:\n",
    "        slop_rank_logger.error(f\"Fatal error in main: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
