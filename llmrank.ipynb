{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Introducing SlopRank\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import llm\n",
    "import networkx as nx\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "##############################################################################\n",
    "# 1. Configuration\n",
    "##############################################################################\n",
    "\n",
    "MODEL_NAMES = [\n",
    "    \"gemini-2.0-flash-thinking-exp-1219\",\n",
    "    \"gemini-exp-1206\",\n",
    "    \"claude-3-5-sonnet-latest\",\n",
    "    \"claude-3-opus-latest\",\n",
    "    \"o1-preview\",\n",
    "    \"gpt-4o\",\n",
    "    \"deepseek-chat\"\n",
    "]\n",
    "\n",
    "model_objects = {m: llm.get_model(m) for m in MODEL_NAMES}\n",
    "\n",
    "EVALUATION_METHOD = 1  # 1 (numeric 1â€“10) or 2 (Upvote/Downvote)\n",
    "USE_SUBSET_EVALUATION = False  # Toggle to use partial evaluation\n",
    "EVALUATORS_SUBSET_SIZE = 2  # If True, limit judges to evaluate a subset of models\n",
    "\n",
    "##############################################################################\n",
    "# 2. Prompting functions\n",
    "##############################################################################\n",
    "\n",
    "def query_model(prompt, model_name):\n",
    "    \"\"\"\n",
    "    Sends a prompt to a specified model via 'llm' and returns the response text.\n",
    "    \"\"\"\n",
    "    response = model_objects[model_name].prompt(prompt)\n",
    "    return response.text()\n",
    "\n",
    "def query_model_all(df, model_name):\n",
    "    \"\"\"\n",
    "    Query the chosen model for all prompts in the DataFrame and save responses.\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    cleaned_prompts = df[\"prompt\"].str.strip().str.lower()\n",
    "    colname = f\"response_{model_name}\"\n",
    "    df[colname] = cleaned_prompts.map(lambda x: query_model(x, model_name))\n",
    "    print(f\"{model_name} processing time: {time.time() - t0:.2f}s\")\n",
    "\n",
    "    # Ensure the 'responses' directory exists\n",
    "    os.makedirs(\"responses\", exist_ok=True)\n",
    "    \n",
    "    # Save responses for this model\n",
    "    response_file_path = f\"responses/responses_{model_name}.csv\"\n",
    "    df[[colname]].to_csv(response_file_path, index=False)\n",
    "    print(f\"Saved responses for {model_name} to {response_file_path}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def gather_all_model_responses(raw_prompts):\n",
    "    \"\"\"\n",
    "    Gather responses from all models and save to disk incrementally.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"prompt\": raw_prompts})\n",
    "    for m in MODEL_NAMES:\n",
    "        df = query_model_all(df, m)\n",
    "    return df\n",
    "\n",
    "##############################################################################\n",
    "# 3. Evaluate the responses\n",
    "##############################################################################\n",
    "\n",
    "def create_evaluation_mapping(other_models):\n",
    "    \"\"\"\n",
    "    Creates a mapping between model names and anonymous identifiers.\n",
    "    Returns both forward and reverse mappings.\n",
    "    \"\"\"\n",
    "    model_to_anonymous = {model: f\"Model_{i+1}\" for i, model in enumerate(other_models)}\n",
    "    anonymous_to_model = {v: k for k, v in model_to_anonymous.items()}\n",
    "    return model_to_anonymous, anonymous_to_model\n",
    "\n",
    "def build_evaluation_prompt(method, prompt, judge_model, model_responses, other_models):\n",
    "    \"\"\"\n",
    "    Build a meta-prompt using anonymous identifiers for models.\n",
    "    \"\"\"\n",
    "    # Create anonymous mapping for this evaluation round\n",
    "    model_to_anonymous, _ = create_evaluation_mapping(other_models)\n",
    "    \n",
    "    answers_section = \"\\n\".join([\n",
    "        f\"{model_to_anonymous[om]}:\\n{model_responses[om]}\\n---\" \n",
    "        for om in other_models\n",
    "    ])\n",
    "\n",
    "    if method == 1:\n",
    "        instructions = f\"\"\"You are evaluating several AI responses to a given prompt. Analyze each response carefully using these criteria:\n",
    "\n",
    "ORIGINAL PROMPT:\n",
    "\"{prompt}\"\n",
    "\n",
    "RESPONSES TO EVALUATE:\n",
    "{answers_section}\n",
    "\n",
    "EVALUATION CRITERIA:\n",
    "1. Accuracy & Correctness (if applicable)\n",
    "2. Completeness of the answer\n",
    "3. Clarity and coherence\n",
    "4. Relevance to the prompt\n",
    "5. Depth of analysis\n",
    "6. Practical usefulness\n",
    "7. Originality of insights (if applicable)\n",
    "\n",
    "RATING SCALE:\n",
    "- 10: Exceptional, exceeds expectations in all criteria\n",
    "- 8-9: Excellent, minor room for improvement\n",
    "- 6-7: Good, satisfactory response\n",
    "- 4-5: Fair, notable shortcomings\n",
    "- 1-3: Poor, significant issues\n",
    "\n",
    "REQUIRED FORMAT:\n",
    "Return ratings in strict JSON format with no additional text:\n",
    "{{\n",
    "    \"Model_1\": 8,\n",
    "    \"Model_2\": 7\n",
    "}}\n",
    "\n",
    "IMPORTANT:\n",
    "- Use whole numbers only (1-10)\n",
    "- Rate each response independently\n",
    "- Focus on substance over style\n",
    "- Consider the specific context of the prompt\n",
    "\"\"\"\n",
    "    else:\n",
    "        instructions = f\"\"\"You are evaluating several AI responses to a given prompt. Analyze each response carefully.\n",
    "\n",
    "ORIGINAL PROMPT:\n",
    "\"{prompt}\"\n",
    "\n",
    "RESPONSES TO EVALUATE:\n",
    "{answers_section}\n",
    "\n",
    "EVALUATION CRITERIA:\n",
    "- Upvote: The response is helpful, accurate, and addresses the prompt effectively\n",
    "- Downvote: The response is inadequate, incorrect, or fails to address the prompt properly\n",
    "\n",
    "Consider:\n",
    "1. Does the response directly answer the prompt?\n",
    "2. Is the information accurate and reliable?\n",
    "3. Is the response clear and well-structured?\n",
    "4. Does it provide practical value?\n",
    "\n",
    "REQUIRED FORMAT:\n",
    "Return votes in strict JSON format with no additional text:\n",
    "{{\n",
    "    \"Model_1\": \"Upvote\",\n",
    "    \"Model_2\": \"Downvote\"\n",
    "}}\n",
    "\n",
    "IMPORTANT:\n",
    "- Be consistent in your evaluation criteria\n",
    "- Focus on the actual value provided\n",
    "- Consider the specific context of the prompt\n",
    "\"\"\"\n",
    "\n",
    "    return instructions.strip(), model_to_anonymous\n",
    "\n",
    "def parse_evaluation_output(method, raw_judgment, anonymous_mapping):\n",
    "    \"\"\"\n",
    "    Parse the raw text and map anonymous identifiers back to model names.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if \"{\" in raw_judgment and \"}\" in raw_judgment:\n",
    "            start = raw_judgment.find(\"{\")\n",
    "            end = raw_judgment.rfind(\"}\") + 1\n",
    "            raw_judgment = raw_judgment[start:end]\n",
    "        \n",
    "        data = json.loads(raw_judgment)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Warning: Failed to parse judgment: {raw_judgment[:100]}...\")\n",
    "        if method == 1:\n",
    "            return {anonymous_mapping[k]: 5.0 for k in anonymous_mapping}\n",
    "        else:\n",
    "            return {anonymous_mapping[k]: 0 for k in anonymous_mapping}\n",
    "\n",
    "    endorsement_map = {}\n",
    "    for anonymous_id, real_model in anonymous_mapping.items():\n",
    "        val = data.get(anonymous_id, None)\n",
    "        \n",
    "        if method == 1:\n",
    "            try:\n",
    "                score = float(val)\n",
    "                if not (1 <= score <= 10):\n",
    "                    print(f\"Warning: Score {score} for {anonymous_id} out of range, clamping to [1,10]\")\n",
    "                endorsement_map[real_model] = max(1.0, min(10.0, score))\n",
    "            except (TypeError, ValueError):\n",
    "                print(f\"Warning: Invalid numeric score for {anonymous_id}: {val}\")\n",
    "                endorsement_map[real_model] = 5.0\n",
    "        else:\n",
    "            if isinstance(val, str):\n",
    "                val = val.lower().strip()\n",
    "                if val == \"upvote\":\n",
    "                    endorsement_map[real_model] = 1\n",
    "                elif val == \"downvote\":\n",
    "                    endorsement_map[real_model] = 0\n",
    "                else:\n",
    "                    print(f\"Warning: Invalid vote value for {anonymous_id}: {val}\")\n",
    "                    endorsement_map[real_model] = 0\n",
    "            else:\n",
    "                print(f\"Warning: Invalid vote type for {anonymous_id}: {type(val)}\")\n",
    "                endorsement_map[real_model] = 0\n",
    "\n",
    "    return endorsement_map\n",
    "\n",
    "def evaluate_responses(df):\n",
    "    \"\"\"\n",
    "    Evaluate responses using anonymous identifiers while maintaining external mapping.\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(MODEL_NAMES)\n",
    "\n",
    "    # Create a DataFrame to store all evaluations with both anonymous and real identifiers\n",
    "    evaluations_df = pd.DataFrame(columns=[\n",
    "        'prompt', \n",
    "        'judge_model', \n",
    "        'rated_model_anonymous', \n",
    "        'rated_model_real',\n",
    "        'rating', \n",
    "        'method'\n",
    "    ])\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        prompt = row[\"prompt\"]\n",
    "        model_responses = {m: row.get(f\"response_{m}\", \"No response\") for m in MODEL_NAMES}\n",
    "\n",
    "        for judge_model in MODEL_NAMES:\n",
    "            other_models = [m for m in MODEL_NAMES if m != judge_model]\n",
    "\n",
    "            if USE_SUBSET_EVALUATION and len(other_models) > EVALUATORS_SUBSET_SIZE:\n",
    "                other_models = random.sample(other_models, EVALUATORS_SUBSET_SIZE)\n",
    "\n",
    "            if not other_models:\n",
    "                continue\n",
    "\n",
    "            evaluation_prompt, model_to_anonymous = build_evaluation_prompt(\n",
    "                EVALUATION_METHOD, prompt, judge_model, model_responses, other_models\n",
    "            )\n",
    "            anonymous_to_model = {v: k for k, v in model_to_anonymous.items()}\n",
    "            \n",
    "            raw_judgment = query_model(evaluation_prompt, judge_model)\n",
    "            parsed_judgments = parse_evaluation_output(\n",
    "                EVALUATION_METHOD, raw_judgment, anonymous_to_model\n",
    "            )\n",
    "\n",
    "            # Store evaluations with both anonymous and real identifiers\n",
    "            for rated_model, endorsement_val in parsed_judgments.items():\n",
    "                anonymous_id = model_to_anonymous[rated_model]\n",
    "                evaluations_df = pd.concat([evaluations_df, pd.DataFrame({\n",
    "                    'prompt': [prompt],\n",
    "                    'judge_model': [judge_model],\n",
    "                    'rated_model_anonymous': [anonymous_id],\n",
    "                    'rated_model_real': [rated_model],\n",
    "                    'rating': [endorsement_val],\n",
    "                    'method': [EVALUATION_METHOD]\n",
    "                })], ignore_index=True)\n",
    "\n",
    "                if G.has_edge(judge_model, rated_model):\n",
    "                    G[judge_model][rated_model][\"weight\"] += endorsement_val\n",
    "                else:\n",
    "                    G.add_edge(judge_model, rated_model, weight=endorsement_val)\n",
    "\n",
    "    # Compute PageRank\n",
    "    pagerank_scores = nx.pagerank(G, weight=\"weight\")\n",
    "    ranked_models = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Save all data\n",
    "    nx.write_gml(G, \"endorsement_graph.gml\")\n",
    "    print(\"Saved endorsement graph to endorsement_graph.gml\")\n",
    "\n",
    "    evaluations_df.to_csv(\"evaluations_with_mapping.csv\", index=False)\n",
    "    print(\"Saved detailed evaluations with mappings to evaluations_with_mapping.csv\")\n",
    "\n",
    "    with open(\"rankings.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"rankings\": ranked_models,\n",
    "            \"metadata\": {\n",
    "                \"evaluation_method\": EVALUATION_METHOD,\n",
    "                \"use_subset_evaluation\": USE_SUBSET_EVALUATION,\n",
    "                \"evaluators_subset_size\": EVALUATORS_SUBSET_SIZE,\n",
    "                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "        }, f, indent=4)\n",
    "    print(\"Saved rankings to rankings.json\")\n",
    "\n",
    "    return G, ranked_models\n",
    "##############################################################################\n",
    "# 4. Main\n",
    "##############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prompts_df = pd.read_csv(\"prompts.csv\")  # Assuming the CSV has a column named \"Questions\"\n",
    "    raw_prompts = prompts_df[\"Questions\"].tolist()\n",
    "    raw_prompts = [prompts_df[\"Questions\"].iloc[0]]  # Use only the first prompt\n",
    "\n",
    "    # Gather responses for each model\n",
    "    df_responses = gather_all_model_responses(raw_prompts)\n",
    "\n",
    "    # Evaluate responses and compute rankings\n",
    "    G, ranked = evaluate_responses(df_responses)\n",
    "\n",
    "    print(\"\\n=== PageRank Scores ===\")\n",
    "    for model, score in ranked:\n",
    "        print(f\"{model}: {score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
