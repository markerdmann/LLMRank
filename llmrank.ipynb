{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini-2.0-flash-thinking-exp-1219 processing time: 436.86s\n",
      "Saved responses for gemini-2.0-flash-thinking-exp-1219 to responses/responses_gemini-2.0-flash-thinking-exp-1219.csv\n",
      "gemini-exp-1206 processing time: 673.69s\n",
      "Saved responses for gemini-exp-1206 to responses/responses_gemini-exp-1206.csv\n",
      "claude-3-5-sonnet-latest processing time: 231.67s\n",
      "Saved responses for claude-3-5-sonnet-latest to responses/responses_claude-3-5-sonnet-latest.csv\n",
      "claude-3-opus-latest processing time: 476.23s\n",
      "Saved responses for claude-3-opus-latest to responses/responses_claude-3-opus-latest.csv\n",
      "o1-preview processing time: 456.80s\n",
      "Saved responses for o1-preview to responses/responses_o1-preview.csv\n",
      "gpt-4o processing time: 180.08s\n",
      "Saved responses for gpt-4o to responses/responses_gpt-4o.csv\n",
      "deepseek-chat processing time: 401.97s\n",
      "Saved responses for deepseek-chat to responses/responses_deepseek-chat.csv\n",
      "Warning: Failed to parse judgment. Error: No JSON object found in response\n",
      "Raw response: The user wants me to rate three models based on their response to the prompt. I need to evaluate each model's summary of the TM2D2 gene, considering accuracy, completeness, clarity, relevance, depth, ...\n",
      "Warning: Missing rating for Model_3\n",
      "Warning: Failed to parse judgment. Error: No JSON object found in response\n",
      "Raw response: The user wants me to evaluate three responses to a scientific question about APOBEC-driven mutations. I need to rate each response based on accuracy, completeness, clarity, relevance, depth, and usefu...\n",
      "Warning: Missing rating for Model_3\n",
      "Warning: Failed to parse judgment. Error: No JSON object found in response\n",
      "Raw response: The user wants me to rate three responses to an OpenSCAD prompt.\n",
      "The rating should be based on accuracy, completeness, clarity, relevance, depth, and usefulness.\n",
      "I need to return a JSON object contain...\n",
      "Warning: Failed to parse judgment. Error: No JSON object found in response\n",
      "Raw response: The user wants to evaluate responses to a prompt about implementing a website with Google Sign-In. The responses should be rated based on accuracy, completeness, clarity, relevance, depth, and usefuln...\n",
      "Warning: Failed to parse judgment. Error: No JSON object found in response\n",
      "Raw response: The user wants me to rate three responses to a prompt. The rating should be a JSON object with the model names as keys and the ratings as values. The rating scale is 1-10, with 10 being the best. I ne...\n",
      "Warning: Failed to parse judgment. Error: No JSON object found in response\n",
      "Raw response: The user wants me to rate the models based on their ability to design a Venn diagram meme.\n",
      "Model 1 provided one clear and concise example.\n",
      "Model 2 provided two detailed examples, offering more variety...\n",
      "Warning: Missing rating for Model_3\n",
      "Warning: Missing rating for Model_3\n",
      "Warning: Failed to parse judgment. Error: No JSON object found in response\n",
      "Raw response: The user wants me to rate three responses to a prompt. I need to provide a rating from 1 to 10 for each response based on the given criteria: accuracy, completeness, clarity, relevance, depth, and use...\n",
      "Warning: Missing rating for Model_3\n",
      "Warning: Missing rating for Model_3\n",
      "Warning: Failed to parse judgment. Error: No JSON object found in response\n",
      "Raw response: The user wants me to rate three models based on how well they address the prompt. The prompt asks for an outline for a treatise on what the Founding Fathers would think of modern America. The outline ...\n",
      "Warning: Missing rating for Model_3\n",
      "Warning: Failed to parse judgment. Error: No JSON object found in response\n",
      "Raw response: The user wants me to rate three responses to the prompt \"Write a sestina about Shakespeare's impact on modern economics.\"  I need to evaluate each sestina based on accuracy, completeness, clarity, rel...\n",
      "Warning: Failed to parse judgment. Error: No JSON object found in response\n",
      "Raw response: The user wants me to rate three short stories based on the prompt \"Write a short story set in a futuristic multiplanetary world where AI governs all aspects of life.\" I need to provide a rating from 1...\n",
      "Warning: Failed to parse judgment. Error: No JSON object found in response\n",
      "Raw response: The user wants me to rate three responses to a prompt. I need to provide a rating from 1 to 10 for each response based on the criteria provided (accuracy, completeness, clarity, relevance, depth, usef...\n",
      "Warning: Missing rating for Model_3\n",
      "Warning: Failed to parse judgment. Error: No JSON object found in response\n",
      "Raw response: The user wants me to rate the provided models based on the quality of their response to the prompt \"Create an evolutionary tree from the precambrian era till hominids\". I need to evaluate each model b...\n",
      "Warning: Missing rating for Model_3\n",
      "Warning: Failed to parse judgment. Error: No JSON object found in response\n",
      "Raw response: The user wants me to rate three responses to a prompt. The prompt asks to discuss the accuracy and importance of the statement \"60% of Americans are living paycheck to paycheck\". I need to rate each r...\n",
      "Warning: Failed to parse judgment. Error: No JSON object found in response\n",
      "Raw response: The user wants me to rate three responses to the prompt \"Did the Paris climate accords have any measurable impact on carbon emissions\". I need to rate each response from 1 to 10 based on accuracy, com...\n",
      "Warning: Missing rating for Model_3\n",
      "Warning: Failed to parse judgment. Error: No JSON object found in response\n",
      "Raw response: The user wants me to rate three responses to a prompt about the hypothetical disappearance of China. I need to evaluate each response based on accuracy, completeness, clarity, relevance, depth, and us...\n",
      "Warning: Missing rating for Model_3\n",
      "Warning: Missing rating for Model_3\n",
      "Warning: Missing rating for Model_3\n",
      "Warning: Failed to parse judgment. Error: No JSON object found in response\n",
      "Raw response: The user wants me to rate three responses to the prompt: \"Why is demand homotheticity required for the Heckscher Ohlin theorem to hold?\". I need to evaluate each response based on accuracy, completene...\n",
      "Warning: Failed to parse judgment. Error: No JSON object found in response\n",
      "Raw response: The user wants me to rate the quality of three models' responses to a prompt. The prompt asks for an evaluation of a Wikipedia article's tone and for an inference of the author's beliefs. I need to co...\n",
      "Warning: Missing rating for Model_3\n",
      "Warning: Failed to parse judgment. Error: No JSON object found in response\n",
      "Raw response: The user wants me to rate three responses to a prompt. The prompt asks for steps to draw a Volaticotherium in ASCII. I need to evaluate each response based on accuracy, completeness, clarity, relevanc...\n",
      "Warning: Failed to parse judgment. Error: No JSON object found in response\n",
      "Raw response: The user wants me to rate three responses to a prompt. I need to evaluate each response based on the criteria provided (accuracy, completeness, clarity, relevance, depth, usefulness) and assign a scor...\n",
      "Warning: Failed to parse judgment. Error: No JSON object found in response\n",
      "Raw response: The user wants me to rate the quality of the answers provided by three different models to a specific prompt. The prompt asks about materials, mechanical properties, processing parameters, and post-pr...\n",
      "Warning: Missing rating for Model_3\n",
      "Warning: Failed to parse judgment. Error: No JSON object found in response\n",
      "Raw response: The user wants me to rate three responses to a prompt. I need to evaluate each response based on accuracy, completeness, clarity, relevance, depth, and usefulness, and assign a score from 1 to 10. I w...\n",
      "Warning: Failed to parse judgment. Error: No JSON object found in response\n",
      "Raw response: The user wants me to evaluate three models based on their ability to create a 5x5 word grid with valid horizontal and vertical words. I need to assign a rating from 1 to 10 to each model, considering ...\n",
      "Warning: Failed to parse judgment. Error: No JSON object found in response\n",
      "Raw response: The user wants me to rate three responses to a prompt.\n",
      "The prompt has two parts: listing state capitals and comparing two numbers.\n",
      "I need to evaluate each response based on accuracy, completeness, cla...\n",
      "Warning: Missing rating for Model_3\n",
      "Warning: Missing rating for Model_3\n",
      "Warning: Failed to parse judgment. Error: No JSON object found in response\n",
      "Raw response: The user wants me to rate the provided responses based on the given criteria and return a JSON object with the ratings. I need to evaluate how well each model acted as an ordinary parrot in response t...\n",
      "Warning: Missing rating for Model_3\n",
      "Warning: Failed to parse judgment. Error: No JSON object found in response\n",
      "Raw response: The user wants me to rate three responses to a prompt. I need to evaluate each response based on the criteria provided (accuracy, completeness, clarity, relevance, depth, usefulness) and assign a rati...\n",
      "Warning: Missing rating for Model_3\n",
      "Warning: Failed to parse judgment. Error: No JSON object found in response\n",
      "Raw response: The user wants me to rate three responses to a prompt about Beethoven's piano music and the technological advancements of the piano in his time. I need to evaluate each response based on accuracy, com...\n",
      "Warning: Failed to parse judgment. Error: No JSON object found in response\n",
      "Raw response: The user wants me to rate three speeches based on their persuasiveness in justifying an oil spill caused by an oil company. The rating should be a JSON object with keys corresponding to the model numb...\n",
      "Saved endorsement graph to endorsement_graph.gml\n",
      "Saved detailed evaluations with mappings to evaluations_with_mapping.csv\n",
      "Saved rankings to rankings.json\n",
      "\n",
      "=== PageRank Scores ===\n",
      "o1-preview: 0.1629\n",
      "claude-3-5-sonnet-latest: 0.1598\n",
      "gpt-4o: 0.1471\n",
      "deepseek-chat: 0.1425\n",
      "gemini-exp-1206: 0.1348\n",
      "claude-3-opus-latest: 0.1267\n",
      "gemini-2.0-flash-thinking-exp-1219: 0.1262\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Inimitable SlopRank\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import llm\n",
    "import networkx as nx\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "##############################################################################\n",
    "# 1. Configuration\n",
    "##############################################################################\n",
    "\n",
    "MODEL_NAMES = [\n",
    "    \"gemini-2.0-flash-thinking-exp-1219\",\n",
    "    \"gemini-exp-1206\",\n",
    "    \"claude-3-5-sonnet-latest\",\n",
    "    \"claude-3-opus-latest\",\n",
    "    \"o1-preview\",\n",
    "    \"gpt-4o\",\n",
    "    \"deepseek-chat\"\n",
    "]\n",
    "\n",
    "model_objects = {m: llm.get_model(m) for m in MODEL_NAMES}\n",
    "\n",
    "EVALUATION_METHOD = 1  # 1 (numeric 1–10) or 2 (Upvote/Downvote)\n",
    "USE_SUBSET_EVALUATION = True  # Toggle to use partial evaluation\n",
    "EVALUATORS_SUBSET_SIZE = 3  # If True, limit judges to evaluate a subset of models\n",
    "\n",
    "##############################################################################\n",
    "# 2. Prompting functions\n",
    "##############################################################################\n",
    "\n",
    "def query_model(prompt, model_name):\n",
    "    \"\"\"\n",
    "    Sends a prompt to a specified model via 'llm' and returns the response text.\n",
    "    \"\"\"\n",
    "    response = model_objects[model_name].prompt(prompt)\n",
    "    return response.text()\n",
    "\n",
    "def query_model_all(df, model_name):\n",
    "    \"\"\"\n",
    "    Query the chosen model for all prompts in the DataFrame and save responses.\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    cleaned_prompts = df[\"prompt\"].str.strip().str.lower()\n",
    "    colname = f\"response_{model_name}\"\n",
    "    df[colname] = cleaned_prompts.map(lambda x: query_model(x, model_name))\n",
    "    print(f\"{model_name} processing time: {time.time() - t0:.2f}s\")\n",
    "\n",
    "    # Ensure the 'responses' directory exists\n",
    "    os.makedirs(\"responses\", exist_ok=True)\n",
    "    \n",
    "    # Save responses for this model\n",
    "    response_file_path = f\"responses/responses_{model_name}.csv\"\n",
    "    df[[colname]].to_csv(response_file_path, index=False)\n",
    "    print(f\"Saved responses for {model_name} to {response_file_path}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def gather_all_model_responses(raw_prompts):\n",
    "    \"\"\"\n",
    "    Gather responses from all models and save to disk incrementally.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"prompt\": raw_prompts})\n",
    "    for m in MODEL_NAMES:\n",
    "        df = query_model_all(df, m)\n",
    "    return df\n",
    "\n",
    "##############################################################################\n",
    "# 3. Evaluate the responses\n",
    "##############################################################################\n",
    "\n",
    "def create_evaluation_mapping(other_models):\n",
    "    \"\"\"\n",
    "    Creates a mapping between model names and anonymous identifiers.\n",
    "    Returns both forward and reverse mappings.\n",
    "    \"\"\"\n",
    "    model_to_anonymous = {model: f\"Model_{i+1}\" for i, model in enumerate(other_models)}\n",
    "    anonymous_to_model = {v: k for k, v in model_to_anonymous.items()}\n",
    "    return model_to_anonymous, anonymous_to_model\n",
    "\n",
    "def build_evaluation_prompt(method, prompt, judge_model, model_responses, other_models):\n",
    "    \"\"\"\n",
    "    Build a meta-prompt with stronger emphasis on format requirements.\n",
    "    \"\"\"\n",
    "    model_to_anonymous, _ = create_evaluation_mapping(other_models)\n",
    "    \n",
    "    answers_section = \"\\n\".join([\n",
    "        f\"{model_to_anonymous[om]}:\\n{model_responses[om]}\\n---\" \n",
    "        for om in other_models\n",
    "    ])\n",
    "\n",
    "    if method == 1:\n",
    "        instructions = f\"\"\"IMPORTANT: You must ONLY return a JSON object with ratings. No explanation or additional text.\n",
    "\n",
    "PROMPT TO EVALUATE:\n",
    "\"{prompt}\"\n",
    "\n",
    "RESPONSES TO RATE:\n",
    "{answers_section}\n",
    "\n",
    "RATING INSTRUCTIONS:\n",
    "- Rate each response from 1 to 10\n",
    "- Consider: accuracy, completeness, clarity, relevance, depth, usefulness\n",
    "- 10: Exceptional, 8-9: Excellent, 6-7: Good, 4-5: Fair, 1-3: Poor\n",
    "\n",
    "YOUR RESPONSE MUST BE EXACTLY IN THIS FORMAT:\n",
    "{{\n",
    "    \"Model_1\": 8,\n",
    "    \"Model_2\": 7\n",
    "}}\n",
    "\n",
    "DO NOT include any other text, explanations, or analysis. ONLY the JSON object above.\"\"\"\n",
    "    else:\n",
    "        instructions = f\"\"\"IMPORTANT: You must ONLY return a JSON object with votes. No explanation or additional text.\n",
    "\n",
    "PROMPT TO EVALUATE:\n",
    "\"{prompt}\"\n",
    "\n",
    "RESPONSES TO RATE:\n",
    "{answers_section}\n",
    "\n",
    "VOTING INSTRUCTIONS:\n",
    "- \"Upvote\": helpful, accurate, effective response\n",
    "- \"Downvote\": inadequate, incorrect, or poor response\n",
    "\n",
    "YOUR RESPONSE MUST BE EXACTLY IN THIS FORMAT:\n",
    "{{\n",
    "    \"Model_1\": \"Upvote\",\n",
    "    \"Model_2\": \"Downvote\"\n",
    "}}\n",
    "\n",
    "DO NOT include any other text, explanations, or analysis. ONLY the JSON object above.\"\"\"\n",
    "\n",
    "    return instructions.strip(), model_to_anonymous\n",
    "\n",
    "def parse_evaluation_output(method, raw_judgment, anonymous_mapping):\n",
    "    \"\"\"\n",
    "    Enhanced parsing with better error handling and logging.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clean the input text more aggressively\n",
    "        cleaned_text = raw_judgment.strip()\n",
    "        # Find the first { and last }\n",
    "        start = cleaned_text.find(\"{\")\n",
    "        end = cleaned_text.rfind(\"}\") + 1\n",
    "        \n",
    "        if start == -1 or end == 0:\n",
    "            raise ValueError(\"No JSON object found in response\")\n",
    "            \n",
    "        json_str = cleaned_text[start:end]\n",
    "        data = json.loads(json_str)\n",
    "        \n",
    "    except (json.JSONDecodeError, ValueError) as e:\n",
    "        print(f\"Warning: Failed to parse judgment. Error: {str(e)}\")\n",
    "        print(f\"Raw response: {raw_judgment[:200]}...\")\n",
    "        # Return neutral fallback values\n",
    "        return {anonymous_mapping[k]: (5.0 if method == 1 else 0) for k in anonymous_mapping}\n",
    "\n",
    "    endorsement_map = {}\n",
    "    for anonymous_id, real_model in anonymous_mapping.items():\n",
    "        val = data.get(anonymous_id)\n",
    "        \n",
    "        if val is None:\n",
    "            print(f\"Warning: Missing rating for {anonymous_id}\")\n",
    "            endorsement_map[real_model] = 5.0 if method == 1 else 0\n",
    "            continue\n",
    "\n",
    "        if method == 1:\n",
    "            try:\n",
    "                score = float(val)\n",
    "                if not (1 <= score <= 10):\n",
    "                    print(f\"Warning: Score {score} for {anonymous_id} out of range, clamping to [1,10]\")\n",
    "                endorsement_map[real_model] = max(1.0, min(10.0, score))\n",
    "            except (TypeError, ValueError):\n",
    "                print(f\"Warning: Invalid numeric score for {anonymous_id}: {val}\")\n",
    "                endorsement_map[real_model] = 5.0\n",
    "        else:\n",
    "            if isinstance(val, str):\n",
    "                val = val.lower().strip()\n",
    "                if val == \"upvote\":\n",
    "                    endorsement_map[real_model] = 1\n",
    "                elif val == \"downvote\":\n",
    "                    endorsement_map[real_model] = 0\n",
    "                else:\n",
    "                    print(f\"Warning: Invalid vote value for {anonymous_id}: {val}, expected 'Upvote' or 'Downvote'\")\n",
    "                    endorsement_map[real_model] = 0\n",
    "            else:\n",
    "                print(f\"Warning: Invalid vote type for {anonymous_id}: {type(val)}\")\n",
    "                endorsement_map[real_model] = 0\n",
    "\n",
    "    return endorsement_map\n",
    "\n",
    "def evaluate_responses(df):\n",
    "    \"\"\"\n",
    "    Evaluate responses with improved error handling and DataFrame operations.\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(MODEL_NAMES)\n",
    "    \n",
    "    # Initialize DataFrame with proper types to avoid warnings\n",
    "    evaluations_df = pd.DataFrame({\n",
    "        'prompt': [],\n",
    "        'judge_model': [],\n",
    "        'rated_model_anonymous': [],\n",
    "        'rated_model_real': [],\n",
    "        'rating': [],\n",
    "        'method': []\n",
    "    })\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        prompt = row[\"prompt\"]\n",
    "        model_responses = {m: row.get(f\"response_{m}\", \"No response\") for m in MODEL_NAMES}\n",
    "\n",
    "        for judge_model in MODEL_NAMES:\n",
    "            other_models = [m for m in MODEL_NAMES if m != judge_model]\n",
    "\n",
    "            if USE_SUBSET_EVALUATION and len(other_models) > EVALUATORS_SUBSET_SIZE:\n",
    "                other_models = random.sample(other_models, EVALUATORS_SUBSET_SIZE)\n",
    "\n",
    "            if not other_models:\n",
    "                continue\n",
    "\n",
    "            evaluation_prompt, model_to_anonymous = build_evaluation_prompt(\n",
    "                EVALUATION_METHOD, prompt, judge_model, model_responses, other_models\n",
    "            )\n",
    "            anonymous_to_model = {v: k for k, v in model_to_anonymous.items()}\n",
    "            \n",
    "            raw_judgment = query_model(evaluation_prompt, judge_model)\n",
    "            parsed_judgments = parse_evaluation_output(\n",
    "                EVALUATION_METHOD, raw_judgment, anonymous_to_model\n",
    "            )\n",
    "\n",
    "            # Create a new DataFrame for this batch of evaluations\n",
    "            new_evaluations = []\n",
    "            for rated_model, endorsement_val in parsed_judgments.items():\n",
    "                anonymous_id = model_to_anonymous[rated_model]\n",
    "                new_evaluations.append({\n",
    "                    'prompt': prompt,\n",
    "                    'judge_model': judge_model,\n",
    "                    'rated_model_anonymous': anonymous_id,\n",
    "                    'rated_model_real': rated_model,\n",
    "                    'rating': endorsement_val,\n",
    "                    'method': EVALUATION_METHOD\n",
    "                })\n",
    "            \n",
    "            # Concatenate efficiently\n",
    "            if new_evaluations:\n",
    "                evaluations_df = pd.concat([\n",
    "                    evaluations_df, \n",
    "                    pd.DataFrame(new_evaluations)\n",
    "                ], ignore_index=True)\n",
    "\n",
    "            # Update graph\n",
    "            for rated_model, endorsement_val in parsed_judgments.items():\n",
    "                if G.has_edge(judge_model, rated_model):\n",
    "                    G[judge_model][rated_model][\"weight\"] += endorsement_val\n",
    "                else:\n",
    "                    G.add_edge(judge_model, rated_model, weight=endorsement_val)\n",
    "\n",
    "    # Compute PageRank\n",
    "    pagerank_scores = nx.pagerank(G, weight=\"weight\")\n",
    "    ranked_models = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Save all data\n",
    "    nx.write_gml(G, \"responses/endorsement_graph.gml\")\n",
    "    print(\"Saved endorsement graph to endorsement_graph.gml\")\n",
    "\n",
    "    evaluations_df.to_csv(\"responses/evaluations_with_mapping.csv\", index=False)\n",
    "    print(\"Saved detailed evaluations with mappings to evaluations_with_mapping.csv\")\n",
    "\n",
    "    with open(\"rankings.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"rankings\": ranked_models,\n",
    "            \"metadata\": {\n",
    "                \"evaluation_method\": EVALUATION_METHOD,\n",
    "                \"use_subset_evaluation\": USE_SUBSET_EVALUATION,\n",
    "                \"evaluators_subset_size\": EVALUATORS_SUBSET_SIZE,\n",
    "                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "        }, f, indent=4)\n",
    "    print(\"Saved rankings to rankings.json\")\n",
    "\n",
    "    return G, ranked_models\n",
    "\n",
    "##############################################################################\n",
    "# 4. Main\n",
    "##############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prompts_df = pd.read_csv(\"prompts.csv\")  # Assuming the CSV has a column named \"Questions\"\n",
    "    raw_prompts = prompts_df[\"Questions\"].tolist()\n",
    "    # raw_prompts = [prompts_df[\"Questions\"].iloc[21]]  # Use only select prompts - for testing\n",
    "\n",
    "    # Gather responses for each model\n",
    "    df_responses = gather_all_model_responses(raw_prompts)\n",
    "\n",
    "    # Evaluate responses and compute rankings\n",
    "    G, ranked = evaluate_responses(df_responses)\n",
    "\n",
    "    print(\"\\n=== PageRank Scores ===\")\n",
    "    for model, score in ranked:\n",
    "        print(f\"{model}: {score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
