{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup config\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING,  # default to WARNING\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"SlopRankLogger\")\n",
    "logger.setLevel(logging.INFO)  # Our SlopRank logs at INFO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Configuration (EvalConfig)\n",
    "\n",
    "@dataclass\n",
    "class EvalConfig:\n",
    "    \"\"\"Configuration for the evaluation system.\"\"\"\n",
    "    model_names: List[str]\n",
    "    evaluation_method: int  # e.g., 1 => numeric rating\n",
    "    use_subset_evaluation: bool\n",
    "    evaluators_subset_size: int\n",
    "    output_dir: Path\n",
    "    request_delay: float = 0.0  # adjustable delay between requests if needed\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        if self.evaluation_method not in {1, 2}:\n",
    "            raise ValueError(\"evaluation_method must be 1 or 2\")\n",
    "        if self.evaluators_subset_size >= len(self.model_names):\n",
    "            raise ValueError(\"evaluators_subset_size must be < number of models\")\n",
    "\n",
    "DEFAULT_CONFIG = EvalConfig(\n",
    "    model_names=[\n",
    "        \"gemini-2.0-flash-thinking-exp-1219\",\n",
    "        \"gemini-exp-1206\",\n",
    "        \"claude-3-5-sonnet-latest\",\n",
    "        \"o1-preview\",\n",
    "        \"gpt-4o\",\n",
    "        \"deepseek-chat\"\n",
    "    ],\n",
    "    evaluation_method=1,  # numeric\n",
    "    use_subset_evaluation=True,\n",
    "    evaluators_subset_size=3,\n",
    "    output_dir=Path(\"results\"),  # folder for CSV outputs\n",
    "    request_delay=0.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 17:42:26,624 - INFO - Reading prompts from prompts.xlsx ...\n",
      "2025-01-13 17:42:26,740 - INFO - Loaded 5 prompts from Excel.\n"
     ]
    }
   ],
   "source": [
    "# 3. Read prompts\n",
    "# We assume you have a local \"prompts.xlsx\" file with columns [\"Questions\", \"Answer_key\"].\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # if you have .env credentials\n",
    "\n",
    "logger.info(\"Reading prompts from prompts.xlsx ...\")\n",
    "prompts_df = pd.read_excel(\"prompts.xlsx\", sheet_name=0)\n",
    "prompts = prompts_df[\"Questions\"].tolist()\n",
    "\n",
    "# If \"Answer_key\" column exists, read it; otherwise fallback to None\n",
    "if \"Answer_key\" in prompts_df.columns:\n",
    "    answer_keys = prompts_df[\"Answer_key\"].tolist()\n",
    "else:\n",
    "    logger.warning(\"No Answer_key column found; using None.\")\n",
    "    answer_keys = [None]*len(prompts_df)\n",
    "\n",
    "prompt_pairs = list(zip(prompts, answer_keys))\n",
    "logger.info(f\"Loaded {len(prompt_pairs)} prompts from Excel.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Collecting the responses (with partial checks)\n",
    "\n",
    "def collect_responses(prompt_pairs: List[Tuple[str, str]], config: EvalConfig, llm_module) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Query each model with each prompt, skipping any (prompt, model) pairs\n",
    "    already found in the existing responses.csv. \n",
    "    Return the combined DataFrame: (prompt, model, response, is_valid, response_time, Answer_key, token_count).\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Collecting responses (with partial coverage check)...\")\n",
    "\n",
    "    # 1) Try to load existing responses\n",
    "    resp_path = config.output_dir / \"responses.csv\"\n",
    "    existing_responses_df = None\n",
    "    if resp_path.exists():\n",
    "        logger.info(f\"Found existing responses at {resp_path}, will skip duplicates.\")\n",
    "        existing_responses_df = pd.read_csv(resp_path)\n",
    "    else:\n",
    "        logger.info(\"No existing responses file found; we'll collect everything from scratch.\")\n",
    "\n",
    "    new_rows = []\n",
    "    total_start = time.time()\n",
    "\n",
    "    # 2) For each (prompt, answer_key) pair\n",
    "    for i, (prompt, answer_key) in enumerate(prompt_pairs, 1):\n",
    "        logger.info(f\"Processing prompt {i}/{len(prompt_pairs)}: {prompt[:60]}...\")\n",
    "        for model_name in config.model_names:\n",
    "            # Skip if we already have a row for (prompt, model_name)\n",
    "            if existing_responses_df is not None:\n",
    "                subset = existing_responses_df[\n",
    "                    (existing_responses_df[\"prompt\"] == prompt) &\n",
    "                    (existing_responses_df[\"model\"] == model_name)\n",
    "                ]\n",
    "                if not subset.empty:\n",
    "                    # Already have it; skip\n",
    "                    logger.info(f\"Skipping existing response for model={model_name}, prompt={prompt[:40]}...\")\n",
    "                    continue\n",
    "\n",
    "            # Otherwise, query the model now\n",
    "            start_time = time.time()\n",
    "            logger.info(f\"Querying {model_name} for new response...\")\n",
    "            try:\n",
    "                model = llm_module.get_model(model_name)\n",
    "                raw_response = model.prompt(prompt).text()\n",
    "\n",
    "                valid = isinstance(raw_response, str) and len(raw_response.strip()) >= 10\n",
    "                elapsed = time.time() - start_time\n",
    "                tokens_used = len(raw_response.split())\n",
    "\n",
    "                new_rows.append({\n",
    "                    'prompt': prompt,\n",
    "                    'model': model_name,\n",
    "                    'response': raw_response if valid else None,\n",
    "                    'is_valid': valid,\n",
    "                    'response_time': elapsed,\n",
    "                    'Answer_key': answer_key,\n",
    "                    'token_count': tokens_used\n",
    "                })\n",
    "                logger.info(\n",
    "                    f\"{model_name} responded in {elapsed:.2f}s - {'Valid' if valid else 'Invalid'}\"\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                elapsed = time.time() - start_time\n",
    "                logger.error(f\"Error from {model_name} after {elapsed:.2f}s: {str(e)}\")\n",
    "                new_rows.append({\n",
    "                    'prompt': prompt,\n",
    "                    'model': model_name,\n",
    "                    'response': None,\n",
    "                    'is_valid': False,\n",
    "                    'response_time': elapsed,\n",
    "                    'Answer_key': answer_key,\n",
    "                    'token_count': 0\n",
    "                })\n",
    "\n",
    "            if config.request_delay > 0.0:\n",
    "                time.sleep(config.request_delay)\n",
    "\n",
    "    total_time = time.time() - total_start\n",
    "    logger.info(f\"Response collection done in {total_time:.2f}s\")\n",
    "\n",
    "    # 3) Combine with existing responses if any\n",
    "    if existing_responses_df is not None:\n",
    "        new_df = pd.DataFrame(new_rows)\n",
    "        combined_df = pd.concat([existing_responses_df, new_df], ignore_index=True)\n",
    "        # Drop duplicates if needed\n",
    "        combined_df.drop_duplicates(subset=[\"prompt\", \"model\"], keep=\"first\", inplace=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        # No prior file => just return new rows\n",
    "        return pd.DataFrame(new_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Collecting Raw Evaluations (Unparsed), with partial checks\n",
    "\n",
    "def collect_raw_evaluations(responses_df: pd.DataFrame, config: EvalConfig, llm_module) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Each model in config.model_names evaluates (rates) the others' responses\n",
    "    but we skip if we already have a row for (prompt, judge_model, model_mapping) \n",
    "    in raw_evaluations.csv. \n",
    "    Returns the combined DataFrame of new + old.\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Collecting raw evaluations (unparsed, partial check)...\")\n",
    "\n",
    "    # 1) Try loading existing raw evaluations\n",
    "    raw_eval_path = config.output_dir / \"raw_evaluations.csv\"\n",
    "    existing_raw_eval_df = None\n",
    "    if raw_eval_path.exists():\n",
    "        logger.info(f\"Found existing raw evaluations at {raw_eval_path}, will skip duplicates.\")\n",
    "        existing_raw_eval_df = pd.read_csv(raw_eval_path)\n",
    "    else:\n",
    "        logger.info(\"No existing raw_evaluations.csv found; collecting from scratch.\")\n",
    "\n",
    "    new_judgments = []\n",
    "\n",
    "    unique_prompts = responses_df['prompt'].unique()\n",
    "    for prompt in unique_prompts:\n",
    "        prompt_subset = responses_df[responses_df['prompt'] == prompt]\n",
    "        answer_key = prompt_subset['Answer_key'].iloc[0] if 'Answer_key' in prompt_subset.columns else None\n",
    "        prompt_responses = prompt_subset.set_index('model')['response'].to_dict()\n",
    "\n",
    "        # Evaluate with each model as judge\n",
    "        for judge_model in config.model_names:\n",
    "            # Exclude judge's own or missing responses\n",
    "            other_models = [\n",
    "                m for m in config.model_names\n",
    "                if m != judge_model and prompt_responses.get(m) is not None\n",
    "            ]\n",
    "            if config.use_subset_evaluation and other_models:\n",
    "                other_models = random.sample(\n",
    "                    other_models,\n",
    "                    min(config.evaluators_subset_size, len(other_models))\n",
    "                )\n",
    "\n",
    "            if not other_models:\n",
    "                continue\n",
    "\n",
    "            # Build the anonymized mapping\n",
    "            model_to_anon = {m: f\"Model_{i+1}\" for i, m in enumerate(other_models)}\n",
    "            answers_section = \"\\n\".join([\n",
    "                f\"{model_to_anon[m]}:\\n{prompt_responses[m]}\\n---\"\n",
    "                for m in other_models\n",
    "            ])\n",
    "            if answer_key:\n",
    "                answer_key_edited = f\"The Answer Key here is:\\n{answer_key}\\n---\\n\"\n",
    "            else:\n",
    "                answer_key_edited = \"\"\n",
    "\n",
    "            instructions = f\"\"\"\n",
    "You are an expert evaluator tasked with assessing the quality of responses from different language models. Your goal is to provide accurate and unbiased ratings based on a given problem, answer key, and set of criteria.\n",
    "\n",
    "First, carefully read the following information:\n",
    "\n",
    "Here is the original problem or prompt:\n",
    "<problem>\n",
    "{prompt}\n",
    "</problem>\n",
    "\n",
    "Here are the answers provided by different models:\n",
    "<answers_section>\n",
    "{answers_section}\n",
    "</answers_section>\n",
    "\n",
    "Here is the answer key to guide your evaluation. It will tell you what could be considered GOOD and BAD, so you can rate appropriately:\n",
    "<answer_key>\n",
    "{answer_key_edited}\n",
    "</answer_key>\n",
    "\n",
    "Your task is to evaluate the answers provided by all the models (Model_1, Model_2, etc.) based on these criteria:\n",
    "1. Accuracy: How well does the answer align with the information as per the answer key?\n",
    "2. Completeness: Does the answer cover all necessary aspects of the problem?\n",
    "3. Clarity: Is the answer easy to understand and well-structured?\n",
    "4. Relevance: Does the answer directly address the given problem?\n",
    "\n",
    "For each model, you will provide a rating on a scale of 1 to 10 for each criterion, where:\n",
    "- 10: Exceptional, world-class, zero errors and all the relevant nuances.\n",
    "- 8-9: Excellent, like a top professional in the field. Not perfect though.\n",
    "- 6-7: Good, like a competent undergraduate student. Doesn't stand out. Average.\n",
    "- 4-5: Fair, like an average high school student. Barely satisfactory.\n",
    "- 1-3: Poor. Factually incorrect and wrong logic.\n",
    "\n",
    "Please follow the following process to evaluate each model:\n",
    "\n",
    "1. Read the problem, answer key, and the model's answer carefully.\n",
    "2. For each criterion:\n",
    "   a. Write down key points from the answer that relate to this criterion.\n",
    "   b. Consider both strengths and weaknesses.\n",
    "   c. Provide a score.\n",
    "3. Calculate an overall score based on the individual criterion scores.\n",
    "4. Format the final rating as a JSON object.\n",
    "\n",
    "Wrap your detailed evaluation for each model in <detailed_evaluation> tags.\n",
    "\n",
    "After evaluating the models, provide your final ratings in a JSON object with the following structure:\n",
    "{{\"Model_1\": X, \"Model_2\": Y}}\n",
    "Where X and Y are integer values between 1 and 10.\n",
    "\n",
    "Remember:\n",
    "- Adhere strictly to the JSON format specified above, i.e., put the response inside a curly bracket.\n",
    "- Provide neutral and accurate ratings based solely on the answer key and the given criteria.\n",
    "- Ensure that your evaluation is thorough and justified.\n",
    "\n",
    "Begin your evaluation now.\"\"\".strip()\n",
    "\n",
    "            # 2) If we already have a row for (prompt, judge_model, model_mapping), skip\n",
    "            model_mapping_str = json.dumps(model_to_anon, sort_keys=True)\n",
    "            already_exists = False\n",
    "            if existing_raw_eval_df is not None:\n",
    "                possible_matches = existing_raw_eval_df[\n",
    "                    (existing_raw_eval_df[\"prompt\"] == prompt) &\n",
    "                    (existing_raw_eval_df[\"judge_model\"] == judge_model)\n",
    "                ]\n",
    "                # Now check if any row has the exact same model_mapping\n",
    "                # We sort_keys=True above so that JSON string is consistent\n",
    "                found_match = possible_matches[\n",
    "                    possible_matches[\"model_mapping\"] == model_mapping_str\n",
    "                ]\n",
    "                if not found_match.empty:\n",
    "                    logger.info(f\"Skipping existing raw eval for judge={judge_model}, prompt={prompt[:40]}...\")\n",
    "                    already_exists = True\n",
    "\n",
    "            if already_exists:\n",
    "                continue\n",
    "\n",
    "            # 3) Otherwise, run the LLM judge\n",
    "            try:\n",
    "                judge_llm = llm_module.get_model(judge_model)\n",
    "                judge_result_obj = judge_llm.prompt(instructions)\n",
    "                raw_judgment = judge_result_obj.text()\n",
    "                raw_judgment_tokens = len(raw_judgment.split())\n",
    "\n",
    "                new_judgments.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"judge_model\": judge_model,\n",
    "                    \"raw_judgment\": raw_judgment,\n",
    "                    # store the same sorted string\n",
    "                    \"model_mapping\": model_mapping_str,\n",
    "                    \"raw_judgment_token_count\": raw_judgment_tokens\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error collecting raw eval from judge={judge_model} on prompt='{prompt}': {str(e)}\")\n",
    "                new_judgments.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"judge_model\": judge_model,\n",
    "                    \"raw_judgment\": None,\n",
    "                    \"model_mapping\": model_mapping_str,\n",
    "                    \"raw_judgment_token_count\": 0,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "\n",
    "    new_eval_df = pd.DataFrame(new_judgments)\n",
    "    logger.info(\"Finished collecting new raw evaluation outputs.\")\n",
    "\n",
    "    # 4) Combine with existing, if any\n",
    "    if existing_raw_eval_df is not None and not new_eval_df.empty:\n",
    "        combined_df = pd.concat([existing_raw_eval_df, new_eval_df], ignore_index=True)\n",
    "        combined_df.drop_duplicates(subset=[\"prompt\", \"judge_model\", \"model_mapping\"], keep=\"first\", inplace=True)\n",
    "        return combined_df\n",
    "    elif existing_raw_eval_df is not None:\n",
    "        # No new data, just return old\n",
    "        return existing_raw_eval_df\n",
    "    else:\n",
    "        # Everything is new\n",
    "        return new_eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Parsing the raw evaluations\n",
    "\n",
    "def parse_evaluation_rows(raw_eval_df: pd.DataFrame, config: EvalConfig) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse each row of the raw_eval_df, which contains judge's raw JSON-like output.\n",
    "    If parsing fails, fallback to a default rating (4.1) for each rated model.\n",
    "    \n",
    "    Returns a DataFrame: (prompt, judge_model, rated_model, score).\n",
    "    \"\"\"\n",
    "    evaluations = []\n",
    "\n",
    "    for _, row in raw_eval_df.iterrows():\n",
    "        prompt = row[\"prompt\"]\n",
    "        judge_model = row[\"judge_model\"]\n",
    "        raw_judgment = row[\"raw_judgment\"]\n",
    "        raw_judgment_tokens = row.get(\"raw_judgment_token_count\", 0)\n",
    "\n",
    "        # Convert model_mapping from JSON string back to dict\n",
    "        try:\n",
    "            model_mapping = json.loads(row[\"model_mapping\"])  # e.g. {\"gemini-exp-1206\":\"Model_1\"}\n",
    "        except:\n",
    "            model_mapping = {}\n",
    "\n",
    "        if not raw_judgment:\n",
    "            # If there's no raw judgment at all, we might skip or fallback\n",
    "            for real_model in model_mapping.keys():\n",
    "                evaluations.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"judge_model\": judge_model,\n",
    "                    \"rated_model\": real_model,\n",
    "                    \"score\": 4.1,           # << changed fallback\n",
    "                    \"parse_failed\": True\n",
    "                })\n",
    "            logger.warning(f\"No raw_judgment for prompt={prompt}, judge={judge_model}; skipping parse.\")\n",
    "            continue\n",
    "\n",
    "        # Try to parse a JSON object from the raw_judgment\n",
    "        try:\n",
    "            start = raw_judgment.find(\"{\")\n",
    "            end = raw_judgment.rfind(\"}\") + 1\n",
    "\n",
    "            if start == -1 or end == 0:\n",
    "                raise ValueError(\"No JSON object found in raw_judgment\")\n",
    "\n",
    "            data = json.loads(raw_judgment[start:end])\n",
    "            # Reverse mapping: \"Model_1\" => \"gemini-exp-1206\"\n",
    "            anon_to_real = {v: k for k, v in model_mapping.items()}\n",
    "\n",
    "            for anon_id, score in data.items():\n",
    "                real_model = anon_to_real.get(anon_id)\n",
    "                if not real_model:\n",
    "                    # If we can't find the real model name, skip\n",
    "                    continue\n",
    "                numeric_score = float(score)\n",
    "                numeric_score = max(1.0, min(10.0, numeric_score))  # clamp 1..10\n",
    "\n",
    "                evaluations.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"judge_model\": judge_model,\n",
    "                    \"rated_model\": real_model,\n",
    "                    \"score\": numeric_score,\n",
    "                    \"parse_failed\": False,\n",
    "                    \"raw_judgment_token_count\": raw_judgment_tokens\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Parsing error for judge={judge_model}, prompt={prompt}: {str(e)}\")\n",
    "            # If parse fails, assign a default rating\n",
    "            for real_model in model_mapping.keys():\n",
    "                evaluations.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"judge_model\": judge_model,\n",
    "                    \"rated_model\": real_model,\n",
    "                    \"score\": 4.1,\n",
    "                    \"parse_failed\": True,\n",
    "                    \"raw_judgment_token_count\": raw_judgment_tokens\n",
    "                })\n",
    "\n",
    "    evals_df = pd.DataFrame(evaluations)\n",
    "    return evals_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 17:42:27,011 - INFO - Using config: EvalConfig(model_names=['gemini-2.0-flash-thinking-exp-1219', 'gemini-exp-1206', 'claude-3-5-sonnet-latest', 'o1-preview', 'gpt-4o', 'deepseek-chat'], evaluation_method=1, use_subset_evaluation=True, evaluators_subset_size=3, output_dir=PosixPath('results'), request_delay=0.0)\n",
      "2025-01-13 17:42:27,011 - INFO - No responses.csv found; collecting now from each model.\n",
      "2025-01-13 17:42:27,012 - INFO - Collecting responses (with partial coverage check)...\n",
      "2025-01-13 17:42:27,012 - INFO - No existing responses file found; we'll collect everything from scratch.\n",
      "2025-01-13 17:42:27,012 - INFO - Processing prompt 1/5: Analyze and compare the architectural styles of the Hagia So...\n",
      "2025-01-13 17:42:27,012 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 17:42:42,819 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 15.81s - Valid\n",
      "2025-01-13 17:42:42,820 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 17:43:10,092 - INFO - gemini-exp-1206 responded in 27.27s - Valid\n",
      "2025-01-13 17:43:10,093 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 17:43:25,758 - INFO - claude-3-5-sonnet-latest responded in 15.67s - Valid\n",
      "2025-01-13 17:43:25,761 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 17:44:03,816 - INFO - o1-preview responded in 38.06s - Valid\n",
      "2025-01-13 17:44:03,817 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 17:44:23,057 - INFO - gpt-4o responded in 19.24s - Valid\n",
      "2025-01-13 17:44:23,058 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 17:44:43,948 - INFO - deepseek-chat responded in 20.89s - Valid\n",
      "2025-01-13 17:44:43,949 - INFO - Processing prompt 2/5: What are the characteristics of APOBEC-driven SGMs, particul...\n",
      "2025-01-13 17:44:43,949 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 17:44:55,412 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 11.46s - Valid\n",
      "2025-01-13 17:44:55,413 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 17:45:17,660 - INFO - gemini-exp-1206 responded in 22.25s - Valid\n",
      "2025-01-13 17:45:17,661 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 17:45:31,891 - INFO - claude-3-5-sonnet-latest responded in 14.23s - Valid\n",
      "2025-01-13 17:45:31,893 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 17:47:11,815 - ERROR - Error from o1-preview after 99.92s: The model produced invalid content. Consider modifying your prompt if you are seeing this error persistently.\n",
      "2025-01-13 17:47:11,818 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 17:47:22,858 - INFO - gpt-4o responded in 11.04s - Valid\n",
      "2025-01-13 17:47:22,859 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 17:47:38,809 - INFO - deepseek-chat responded in 15.95s - Valid\n",
      "2025-01-13 17:47:38,810 - INFO - Processing prompt 3/5: Draft a one-page product requirements document (PRD) for int...\n",
      "2025-01-13 17:47:38,811 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 17:47:52,158 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 13.35s - Valid\n",
      "2025-01-13 17:47:52,159 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 17:48:14,182 - INFO - gemini-exp-1206 responded in 22.02s - Valid\n",
      "2025-01-13 17:48:14,184 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 17:48:25,410 - INFO - claude-3-5-sonnet-latest responded in 11.23s - Valid\n",
      "2025-01-13 17:48:25,412 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 17:48:46,091 - INFO - o1-preview responded in 20.68s - Valid\n",
      "2025-01-13 17:48:46,092 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 17:49:02,631 - INFO - gpt-4o responded in 16.54s - Valid\n",
      "2025-01-13 17:49:02,632 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 17:49:21,778 - INFO - deepseek-chat responded in 19.15s - Valid\n",
      "2025-01-13 17:49:21,779 - INFO - Processing prompt 4/5: Build a google sign in page that takes me to a profile page ...\n",
      "2025-01-13 17:49:21,780 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 17:49:41,190 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 19.41s - Valid\n",
      "2025-01-13 17:49:41,192 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 17:50:51,346 - INFO - gemini-exp-1206 responded in 70.15s - Valid\n",
      "2025-01-13 17:50:51,347 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 17:51:23,917 - INFO - claude-3-5-sonnet-latest responded in 32.57s - Valid\n",
      "2025-01-13 17:51:23,918 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 17:52:09,004 - INFO - o1-preview responded in 45.09s - Valid\n",
      "2025-01-13 17:52:09,004 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 17:52:32,565 - INFO - gpt-4o responded in 23.56s - Valid\n",
      "2025-01-13 17:52:32,565 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 17:53:02,950 - INFO - deepseek-chat responded in 30.38s - Valid\n",
      "2025-01-13 17:53:02,951 - INFO - Processing prompt 5/5: Can you design a Venn diagram meme that humorously illustrat...\n",
      "2025-01-13 17:53:02,951 - INFO - Querying gemini-2.0-flash-thinking-exp-1219 for new response...\n",
      "2025-01-13 17:53:10,396 - INFO - gemini-2.0-flash-thinking-exp-1219 responded in 7.44s - Valid\n",
      "2025-01-13 17:53:10,396 - INFO - Querying gemini-exp-1206 for new response...\n",
      "2025-01-13 17:53:26,572 - INFO - gemini-exp-1206 responded in 16.18s - Valid\n",
      "2025-01-13 17:53:26,574 - INFO - Querying claude-3-5-sonnet-latest for new response...\n",
      "2025-01-13 17:53:28,560 - INFO - claude-3-5-sonnet-latest responded in 1.99s - Valid\n",
      "2025-01-13 17:53:28,562 - INFO - Querying o1-preview for new response...\n",
      "2025-01-13 17:53:58,403 - INFO - o1-preview responded in 29.84s - Valid\n",
      "2025-01-13 17:53:58,405 - INFO - Querying gpt-4o for new response...\n",
      "2025-01-13 17:54:04,198 - INFO - gpt-4o responded in 5.79s - Valid\n",
      "2025-01-13 17:54:04,199 - INFO - Querying deepseek-chat for new response...\n",
      "2025-01-13 17:54:12,205 - INFO - deepseek-chat responded in 8.01s - Valid\n",
      "2025-01-13 17:54:12,206 - INFO - Response collection done in 705.19s\n",
      "2025-01-13 17:54:12,223 - INFO - Saved new responses to results/responses.csv\n"
     ]
    }
   ],
   "source": [
    "# 7. Full workflow\n",
    "import llm  # custom LLM module\n",
    "\n",
    "# 1) Create a config\n",
    "config = DEFAULT_CONFIG\n",
    "logger.info(f\"Using config: {config}\")\n",
    "\n",
    "# 2) Collect or load responses\n",
    "resp_path = config.output_dir / \"responses.csv\"\n",
    "\n",
    "if resp_path.exists():\n",
    "    logger.info(f\"Loading existing responses from {resp_path}\")\n",
    "    responses_df = pd.read_csv(resp_path)\n",
    "else:\n",
    "    logger.info(\"No responses.csv found; collecting now from each model.\")\n",
    "    responses_df = collect_responses(prompt_pairs, config, llm)\n",
    "    responses_df.to_csv(resp_path, index=False)\n",
    "    logger.info(f\"Saved new responses to {resp_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 17:54:12,234 - INFO - No raw_evaluations.csv found; collecting now (unparsed).\n",
      "2025-01-13 17:54:12,235 - INFO - Collecting raw evaluations (unparsed, partial check)...\n",
      "2025-01-13 17:54:12,235 - INFO - No existing raw_evaluations.csv found; collecting from scratch.\n",
      "2025-01-13 18:04:59,908 - INFO - Finished collecting new raw evaluation outputs.\n",
      "2025-01-13 18:04:59,915 - INFO - Saved raw evaluations to results/raw_evaluations.csv\n"
     ]
    }
   ],
   "source": [
    "# 8. Collect or load raw evaluations\n",
    "raw_eval_path = config.output_dir / \"raw_evaluations.csv\"\n",
    "\n",
    "if raw_eval_path.exists():\n",
    "    logger.info(f\"Loading existing raw evaluations from {raw_eval_path}\")\n",
    "    raw_eval_df = pd.read_csv(raw_eval_path)\n",
    "else:\n",
    "    logger.info(\"No raw_evaluations.csv found; collecting now (unparsed).\")\n",
    "    raw_eval_df = collect_raw_evaluations(responses_df, config, llm)\n",
    "    raw_eval_df.to_csv(raw_eval_path, index=False)\n",
    "    logger.info(f\"Saved raw evaluations to {raw_eval_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 18:04:59,928 - INFO - No evaluations.csv found; parsing raw evaluations now.\n",
      "2025-01-13 18:04:59,930 - ERROR - Parsing error for judge=gemini-2.0-flash-thinking-exp-1219, prompt=Draft a one-page product requirements document (PRD) for integrating a brilliant new AI feature that talks to  to an enterprise software company: No JSON object found in raw_judgment\n",
      "2025-01-13 18:04:59,933 - INFO - Saved parsed evaluations to results/evaluations.csv\n",
      "2025-01-13 18:04:59,934 - INFO - Here are the first few rows of the parsed evaluations:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>judge_model</th>\n",
       "      <th>rated_model</th>\n",
       "      <th>score</th>\n",
       "      <th>parse_failed</th>\n",
       "      <th>raw_judgment_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Analyze and compare the architectural styles o...</td>\n",
       "      <td>gemini-2.0-flash-thinking-exp-1219</td>\n",
       "      <td>deepseek-chat</td>\n",
       "      <td>8.0</td>\n",
       "      <td>False</td>\n",
       "      <td>686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Analyze and compare the architectural styles o...</td>\n",
       "      <td>gemini-2.0-flash-thinking-exp-1219</td>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>8.0</td>\n",
       "      <td>False</td>\n",
       "      <td>686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Analyze and compare the architectural styles o...</td>\n",
       "      <td>gemini-exp-1206</td>\n",
       "      <td>o1-preview</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Analyze and compare the architectural styles o...</td>\n",
       "      <td>gemini-exp-1206</td>\n",
       "      <td>deepseek-chat</td>\n",
       "      <td>8.0</td>\n",
       "      <td>False</td>\n",
       "      <td>590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Analyze and compare the architectural styles o...</td>\n",
       "      <td>gemini-exp-1206</td>\n",
       "      <td>gemini-2.0-flash-thinking-exp-1219</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Analyze and compare the architectural styles o...   \n",
       "1  Analyze and compare the architectural styles o...   \n",
       "2  Analyze and compare the architectural styles o...   \n",
       "3  Analyze and compare the architectural styles o...   \n",
       "4  Analyze and compare the architectural styles o...   \n",
       "\n",
       "                          judge_model                         rated_model  \\\n",
       "0  gemini-2.0-flash-thinking-exp-1219                       deepseek-chat   \n",
       "1  gemini-2.0-flash-thinking-exp-1219                              gpt-4o   \n",
       "2                     gemini-exp-1206                          o1-preview   \n",
       "3                     gemini-exp-1206                       deepseek-chat   \n",
       "4                     gemini-exp-1206  gemini-2.0-flash-thinking-exp-1219   \n",
       "\n",
       "   score  parse_failed  raw_judgment_token_count  \n",
       "0    8.0         False                       686  \n",
       "1    8.0         False                       686  \n",
       "2   10.0         False                       590  \n",
       "3    8.0         False                       590  \n",
       "4    1.0         False                       590  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 9. Parse or load final evaluations\n",
    "eval_path = config.output_dir / \"evaluations.csv\"\n",
    "\n",
    "if eval_path.exists():\n",
    "    logger.info(f\"Loading parsed evaluations from {eval_path}\")\n",
    "    evaluations_df = pd.read_csv(eval_path)\n",
    "else:\n",
    "    logger.info(\"No evaluations.csv found; parsing raw evaluations now.\")\n",
    "    evaluations_df = parse_evaluation_rows(raw_eval_df, config)\n",
    "    evaluations_df.to_csv(eval_path, index=False)\n",
    "    logger.info(f\"Saved parsed evaluations to {eval_path}\")\n",
    "\n",
    "\n",
    "# 10. Inspect or analyze the final numeric scores\n",
    "logger.info(\"Here are the first few rows of the parsed evaluations:\")\n",
    "display(evaluations_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 20:49:50,660 - INFO - === PageRank Results ===\n",
      "2025-01-13 20:49:50,660 - INFO - gemini-exp-1206: 0.2180\n",
      "2025-01-13 20:49:50,661 - INFO - gpt-4o: 0.1933\n",
      "2025-01-13 20:49:50,661 - INFO - deepseek-chat: 0.1771\n",
      "2025-01-13 20:49:50,661 - INFO - o1-preview: 0.1575\n",
      "2025-01-13 20:49:50,661 - INFO - claude-3-5-sonnet-latest: 0.1300\n",
      "2025-01-13 20:49:50,662 - INFO - gemini-2.0-flash-thinking-exp-1219: 0.1240\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>pagerank_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gemini-exp-1206</td>\n",
       "      <td>0.218007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-4o</td>\n",
       "      <td>0.193312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deepseek-chat</td>\n",
       "      <td>0.177139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>o1-preview</td>\n",
       "      <td>0.157507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>claude-3-5-sonnet-latest</td>\n",
       "      <td>0.130013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gemini-2.0-flash-thinking-exp-1219</td>\n",
       "      <td>0.124021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                model  pagerank_score\n",
       "0                     gemini-exp-1206        0.218007\n",
       "1                              gpt-4o        0.193312\n",
       "2                       deepseek-chat        0.177139\n",
       "3                          o1-preview        0.157507\n",
       "4            claude-3-5-sonnet-latest        0.130013\n",
       "5  gemini-2.0-flash-thinking-exp-1219        0.124021"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 11. Build a graph from evaluations, run PageRank, and display final rankings\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "def build_endorsement_graph(evaluations_df: pd.DataFrame, config: EvalConfig) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Builds a directed graph from the numeric evaluations.\n",
    "    Edge: judge_model -> rated_model, weighted by 'score'.\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    # Ensure all models appear as nodes, even if no edges\n",
    "    G.add_nodes_from(config.model_names)\n",
    "\n",
    "    for _, row in evaluations_df.iterrows():\n",
    "        judge = row[\"judge_model\"]\n",
    "        rated = row[\"rated_model\"]\n",
    "        score = float(row[\"score\"])\n",
    "\n",
    "        # Add (judge -> rated, weight=score)\n",
    "        if G.has_edge(judge, rated):\n",
    "            G[judge][rated][\"weight\"] += score\n",
    "        else:\n",
    "            G.add_edge(judge, rated, weight=score)\n",
    "\n",
    "    return G\n",
    "\n",
    "# Now build the graph\n",
    "G = build_endorsement_graph(evaluations_df, config)\n",
    "\n",
    "if len(G.edges) == 0:\n",
    "    logger.warning(\"No edges in the endorsement graph. Nothing to PageRank.\")\n",
    "else:\n",
    "    # Compute PageRank\n",
    "    pagerank_scores = nx.pagerank(G, weight=\"weight\")\n",
    "    # Sort models from highest to lowest PageRank\n",
    "    ranked_models = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    logger.info(\"=== PageRank Results ===\")\n",
    "    for model, score in ranked_models:\n",
    "        logger.info(f\"{model}: {score:.4f}\")\n",
    "\n",
    "    # Optionally display or store in a file\n",
    "    display(pd.DataFrame(ranked_models, columns=[\"model\", \"pagerank_score\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
