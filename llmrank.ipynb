{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini-2.0-flash-thinking-exp-1219 processing time: 25.186904668807983\n",
      "claude-3-5-sonnet-latest processing time: 9.704782009124756\n",
      "o1-preview processing time: 36.931774854660034\n",
      "deepseek-chat processing time: 15.889774799346924\n",
      "\n",
      "=== PageRank Scores ===\n",
      "gemini-2.0-flash-thinking-exp-1219: 0.2563\n",
      "deepseek-chat: 0.2508\n",
      "o1-preview: 0.2474\n",
      "claude-3-5-sonnet-latest: 0.2455\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SlopRank)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import llm\n",
    "import networkx as nx\n",
    "import json\n",
    "import random\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "##############################################################################\n",
    "# 1. Configuration\n",
    "##############################################################################\n",
    "\n",
    "MODEL_NAMES = [\n",
    "    \"gemini-2.0-flash-thinking-exp-1219\",\n",
    "    \"claude-3-5-sonnet-latest\",\n",
    "    \"o1-preview\",\n",
    "    \"deepseek-chat\"\n",
    "]\n",
    "\n",
    "model_objects = {m: llm.get_model(m) for m in MODEL_NAMES}\n",
    "\n",
    "EVALUATION_METHOD = 1  # 1 (numeric 1–10) or 2 (Upvote/Downvote)\n",
    "USE_SUBSET_EVALUATION = False  # Toggle to use partial evaluation\n",
    "EVALUATORS_SUBSET_SIZE = 2    # e.g., each judge only evaluates 2 other models, an option to evaluate only a subset of other models to reduce API calls.\n",
    "\n",
    "##############################################################################\n",
    "# 2. Prompting functions\n",
    "##############################################################################\n",
    "\n",
    "def query_model(prompt, model_name):\n",
    "    \"\"\"\n",
    "    Sends a prompt to a specified model via 'llm' and returns the response text.\n",
    "    \"\"\"\n",
    "    response = model_objects[model_name].prompt(prompt)\n",
    "    return response.text()\n",
    "\n",
    "def query_model_all(df, model_name):\n",
    "    \"\"\"\n",
    "    Clean up prompts, query the chosen model for all prompts in the DataFrame,\n",
    "    store the responses in a new column \"response_{model_name}\".\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    cleaned_prompts = df[\"prompt\"].str.strip().str.lower()  # Example: simple transform\n",
    "    colname = f\"response_{model_name}\"\n",
    "    df[colname] = cleaned_prompts.map(lambda x: query_model(x, model_name))\n",
    "    print(f\"{model_name} processing time:\", time.time() - t0)\n",
    "    return df\n",
    "\n",
    "def gather_all_model_responses(raw_prompts):\n",
    "    \"\"\"\n",
    "    For each model in MODEL_NAMES, gather responses to raw_prompts into a single DataFrame.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\"prompt\": raw_prompts})\n",
    "    for m in MODEL_NAMES:\n",
    "        df = query_model_all(df, m)\n",
    "    return df\n",
    "\n",
    "##############################################################################\n",
    "# 3. Evaluate the responses\n",
    "##############################################################################\n",
    "\n",
    "def evaluate_responses(df):\n",
    "    \"\"\"\n",
    "    Each model will evaluate the other models' answers and produce endorsements.\n",
    "    We'll build a graph of endorsements and run PageRank.\n",
    "\n",
    "    EVALUATION_METHOD=1 -> numeric rating\n",
    "    EVALUATION_METHOD=2 -> upvote/downvote\n",
    "\n",
    "    If USE_SUBSET_EVALUATION=True, each judge evaluates only a random subset\n",
    "    (of size EVALUATORS_SUBSET_SIZE) of the other models.\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(MODEL_NAMES)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        prompt = row[\"prompt\"]\n",
    "        # Gather each model's response for this prompt\n",
    "        model_responses = {\n",
    "            m: row.get(f\"response_{m}\", \"No response\")\n",
    "            for m in MODEL_NAMES\n",
    "        }\n",
    "\n",
    "        # For each judge model, gather endorsements\n",
    "        for judge_model in MODEL_NAMES:\n",
    "            # Which models are we evaluating?\n",
    "            other_models = [m for m in MODEL_NAMES if m != judge_model]\n",
    "\n",
    "            if USE_SUBSET_EVALUATION and len(other_models) > EVALUATORS_SUBSET_SIZE:\n",
    "                # Randomly pick a smaller subset\n",
    "                other_models = random.sample(other_models, EVALUATORS_SUBSET_SIZE)\n",
    "\n",
    "            # If there are none left (e.g. M=1?), skip\n",
    "            if not other_models:\n",
    "                continue\n",
    "\n",
    "            # Build the evaluation prompt\n",
    "            evaluation_prompt = build_evaluation_prompt(\n",
    "                EVALUATION_METHOD, prompt, judge_model, model_responses, other_models\n",
    "            )\n",
    "            # Query judge model\n",
    "            raw_judgment = query_model(evaluation_prompt, judge_model)\n",
    "            # Parse\n",
    "            parsed_judgments = parse_evaluation_output(\n",
    "                EVALUATION_METHOD, raw_judgment, other_models\n",
    "            )\n",
    "\n",
    "            # Add edges in the graph\n",
    "            for rated_model, endorsement_val in parsed_judgments.items():\n",
    "                if G.has_edge(judge_model, rated_model):\n",
    "                    G[judge_model][rated_model][\"weight\"] += endorsement_val\n",
    "                else:\n",
    "                    G.add_edge(judge_model, rated_model, weight=endorsement_val)\n",
    "\n",
    "    # Compute PageRank\n",
    "    pagerank_scores = nx.pagerank(G, weight=\"weight\")\n",
    "    ranked_models = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return G, ranked_models\n",
    "\n",
    "def build_evaluation_prompt(method, prompt, judge_model, model_responses, other_models):\n",
    "    \"\"\"\n",
    "    Build a meta-prompt that instructs 'judge_model' to evaluate the other models' answers.\n",
    "    \"\"\"\n",
    "    # We'll ask for a specific JSON structure.\n",
    "    answers_section = \"\"\n",
    "    for om in other_models:\n",
    "        answers_section += f\"**Model {om}**: {model_responses[om]}\\n\\n\"\n",
    "\n",
    "    if method == 1:\n",
    "        # Numeric rating approach\n",
    "        instructions = f\"\"\"\n",
    "You are {judge_model}. You see the following question:\n",
    "PROMPT: \"{prompt}\"\n",
    "\n",
    "Here are other models' answers:\n",
    "\n",
    "{answers_section}\n",
    "\n",
    "Give each model a rating from 1 to 10, strictly in JSON format with no extra text.\n",
    "Example:\n",
    "{{\n",
    "    \"modelA\": 8,\n",
    "    \"modelB\": 5\n",
    "}}\n",
    "Use each model's name as the key.\n",
    "        \"\"\"\n",
    "    else:\n",
    "        # Upvote/Downvote approach\n",
    "        instructions = f\"\"\"\n",
    "You are {judge_model}. You see the following question:\n",
    "PROMPT: \"{prompt}\"\n",
    "\n",
    "Here are other models' answers:\n",
    "\n",
    "{answers_section}\n",
    "\n",
    "Simply say \"Upvote\" or \"Downvote\" for each model, in JSON format, no extra text.\n",
    "Example:\n",
    "{{\n",
    "    \"modelA\": \"Upvote\",\n",
    "    \"modelB\": \"Downvote\"\n",
    "}}\n",
    "        \"\"\"\n",
    "    return instructions.strip()\n",
    "\n",
    "def parse_evaluation_output(method, raw_judgment, other_models):\n",
    "    \"\"\"\n",
    "    Convert the raw text from the judge model into a dict of endorsements.\n",
    "    method=1 -> numeric 1–10\n",
    "    method=2 -> upvote/downvote\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = json.loads(raw_judgment)\n",
    "    except:\n",
    "        if method == 1:\n",
    "            return {m: 5.0 for m in other_models}  # fallback\n",
    "        else:\n",
    "            return {m: 0 for m in other_models}    # fallback\n",
    "\n",
    "    endorsement_map = {}\n",
    "    for m in other_models:\n",
    "        val = data.get(m, None)\n",
    "        if method == 1:\n",
    "            try:\n",
    "                score = float(val)\n",
    "                if score < 1:\n",
    "                    score = 1\n",
    "                if score > 10:\n",
    "                    score = 10\n",
    "                endorsement_map[m] = score\n",
    "            except:\n",
    "                endorsement_map[m] = 5.0\n",
    "        else:\n",
    "            # Upvote/Downvote\n",
    "            if isinstance(val, str) and val.lower().strip() == \"upvote\":\n",
    "                endorsement_map[m] = 1\n",
    "            elif isinstance(val, str) and val.lower().strip() == \"downvote\":\n",
    "                endorsement_map[m] = 0\n",
    "            else:\n",
    "                endorsement_map[m] = 0\n",
    "\n",
    "    return endorsement_map\n",
    "\n",
    "##############################################################################\n",
    "# 4. Main\n",
    "##############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    raw_prompts = [\n",
    "        \"Name the state capitals of states starting with 'C'. Then tell me what's bigger, 9.11 or 9.9?\",\n",
    "        \"What is the meaning of life?\",\n",
    "        \"Write a poem about Shakespeare's impact on modern economics.\"\n",
    "    ]\n",
    "\n",
    "    # 1) Gather responses for each model\n",
    "    df_responses = gather_all_model_responses(raw_prompts)\n",
    "\n",
    "    # 2) Evaluate (endorse) each other's answers and compute PageRank\n",
    "    G, ranked = evaluate_responses(df_responses)\n",
    "\n",
    "    print(\"\\n=== PageRank Scores ===\")\n",
    "    for model, score in ranked:\n",
    "        print(f\"{model}: {score:.4f}\")\n",
    "\n",
    "    # Inspect edges if you want:\n",
    "    # for edge in G.edges(data=True):\n",
    "    #     print(edge)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
